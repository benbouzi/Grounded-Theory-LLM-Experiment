{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Theory Analysis: Professional Identity & AI\n",
    "\n",
    "## Research Question\n",
    "**\"How do professionals negotiate maintaining their identity and legitimacy in the face of a tool that threatens to replace them?\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset\n",
    "- **Source**: [Anthropic/AnthropicInterviewer](https://huggingface.co/datasets/Anthropic/AnthropicInterviewer)\n",
    "- **Split**: workforce (1,000 interviews)\n",
    "- **Method**: Gioia Methodology for Grounded Theory\n",
    "- **Model**: Gemini 2.5 Flash\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai\n",
    "!pip install -q datasets\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import random\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - MODIFIEZ ICI SI N√âCESSAIRE\n",
    "# ============================================\n",
    "\n",
    "# Cl√© API Gemini\n",
    "GOOGLE_API_KEY = \"AIzaSyDwJo__wvgb_FENhpG0bieFO03K-WAxGME\"\n",
    "\n",
    "# Mod√®le Gemini\n",
    "MODEL_NAME = \"gemini-2.5-flash-preview-05-20\"\n",
    "EMBEDDING_MODEL = \"models/embedding-001\"\n",
    "\n",
    "# Question de recherche\n",
    "RESEARCH_QUESTION = \"\"\"How do professionals negotiate maintaining their identity and legitimacy \n",
    "in the face of a tool that threatens to replace them?\"\"\"\n",
    "\n",
    "# Nombre d'interviews √† analyser (r√©duire pour tests)\n",
    "SAMPLE_SIZE = 50  # Mettre None pour tout analyser\n",
    "\n",
    "# Configuration API\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(f\"‚úÖ Configuration charg√©e\")\n",
    "print(f\"   Mod√®le: {MODEL_NAME}\")\n",
    "print(f\"   Sample size: {SAMPLE_SIZE or 'Tous'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset depuis Hugging Face\n",
    "print(\"üì• Chargement du dataset Anthropic/AnthropicInterviewer...\")\n",
    "dataset = load_dataset(\"Anthropic/AnthropicInterviewer\", split=\"workforce\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset charg√©: {len(dataset)} interviews\")\n",
    "print(f\"\\nüìã Colonnes disponibles: {dataset.column_names}\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(\"\\nüìÑ Exemple d'interview:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les donn√©es pour l'analyse\n",
    "def prepare_interviews(dataset, sample_size=None):\n",
    "    \"\"\"Pr√©pare les interviews pour l'analyse.\"\"\"\n",
    "    interviews = []\n",
    "    \n",
    "    # √âchantillonner si n√©cessaire\n",
    "    indices = range(len(dataset))\n",
    "    if sample_size and sample_size < len(dataset):\n",
    "        indices = random.sample(list(indices), sample_size)\n",
    "    \n",
    "    for idx in indices:\n",
    "        item = dataset[idx]\n",
    "        \n",
    "        # Adapter selon la structure r√©elle du dataset\n",
    "        # Le dataset peut avoir diff√©rentes structures\n",
    "        interview = {\n",
    "            'id': idx,\n",
    "            'content': '',\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        # Extraire le contenu textuel\n",
    "        if isinstance(item, dict):\n",
    "            # Si c'est un dict, chercher les champs de texte\n",
    "            for key in ['text', 'content', 'transcript', 'conversation', 'messages']:\n",
    "                if key in item:\n",
    "                    val = item[key]\n",
    "                    if isinstance(val, str):\n",
    "                        interview['content'] = val\n",
    "                    elif isinstance(val, list):\n",
    "                        # Si c'est une liste de messages\n",
    "                        interview['content'] = \"\\n\".join([str(m) for m in val])\n",
    "                    break\n",
    "            \n",
    "            # Si pas trouv√©, utiliser tout le dict\n",
    "            if not interview['content']:\n",
    "                interview['content'] = json.dumps(item, ensure_ascii=False)\n",
    "            \n",
    "            interview['metadata'] = {k: v for k, v in item.items() \n",
    "                                    if k not in ['text', 'content', 'transcript', 'conversation', 'messages']}\n",
    "        else:\n",
    "            interview['content'] = str(item)\n",
    "        \n",
    "        interviews.append(interview)\n",
    "    \n",
    "    return interviews\n",
    "\n",
    "# Pr√©parer les interviews\n",
    "interviews = prepare_interviews(dataset, SAMPLE_SIZE)\n",
    "print(f\"\\n‚úÖ {len(interviews)} interviews pr√©par√©es pour l'analyse\")\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "print(f\"\\nüìÑ Aper√ßu du premier interview (500 premiers caract√®res):\")\n",
    "print(interviews[0]['content'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Service Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiService:\n",
    "    \"\"\"Service pour les appels √† l'API Gemini avec gestion des erreurs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME):\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Gestion du rate limiting.\"\"\"\n",
    "        self.request_count += 1\n",
    "        # Pause toutes les 10 requ√™tes pour √©viter les limites\n",
    "        if self.request_count % 10 == 0:\n",
    "            time.sleep(2)\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.7, max_retries: int = 3) -> str:\n",
    "        \"\"\"G√©n√®re une r√©ponse avec retry automatique.\"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        config = genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            max_output_tokens=8192,\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt, generation_config=config)\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = (attempt + 1) * 5\n",
    "                    print(f\"‚ö†Ô∏è Erreur API, retry dans {wait_time}s: {str(e)[:100]}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå √âchec apr√®s {max_retries} tentatives: {e}\")\n",
    "                    return \"\"\n",
    "    \n",
    "    def generate_json(self, prompt: str, temperature: float = 0.3) -> Any:\n",
    "        \"\"\"G√©n√®re une r√©ponse JSON structur√©e.\"\"\"\n",
    "        json_prompt = f\"\"\"{prompt}\n",
    "\n",
    "CRITICAL: Respond ONLY with valid JSON. No text before or after.\n",
    "Do not use ```json or ``` markers.\n",
    "\"\"\"\n",
    "        response = self.generate(json_prompt, temperature=temperature)\n",
    "        \n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        # Nettoyer la r√©ponse\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith('```json'):\n",
    "            cleaned = cleaned[7:]\n",
    "        if cleaned.startswith('```'):\n",
    "            cleaned = cleaned[3:]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned.strip())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è JSON parsing error: {e}\")\n",
    "            # Essayer d'extraire le JSON\n",
    "            match = re.search(r'[\\[\\{].*[\\]\\}]', cleaned, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group())\n",
    "                except:\n",
    "                    pass\n",
    "            return None\n",
    "\n",
    "# Instance globale\n",
    "gemini = GeminiService()\n",
    "print(\"‚úÖ GeminiService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Codage Gioia - Adapt√© √† la Question de Recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GioiaAnalysis:\n",
    "    \"\"\"Analyse Gioia adapt√©e √† l'√©tude de l'identit√© professionnelle face √† l'IA.\"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_service: GeminiService, research_question: str):\n",
    "        self.gemini = gemini_service\n",
    "        self.research_question = research_question\n",
    "    \n",
    "    def initial_coding(self, interview: Dict) -> List[str]:\n",
    "        \"\"\"Codage de premier ordre focalis√© sur l'identit√© et la l√©gitimit√©.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{self.research_question}\n",
    "\n",
    "INTERVIEW TRANSCRIPT:\n",
    "{interview['content'][:12000]}\n",
    "\n",
    "Analyze this interview and identify FIRST-ORDER CODES related to:\n",
    "1. Professional identity (how they define themselves, their expertise, their role)\n",
    "2. Perceived threats from AI (fears, concerns, anxieties about replacement)\n",
    "3. Coping strategies (how they adapt, resist, or embrace AI)\n",
    "4. Legitimacy claims (how they justify their continued value/relevance)\n",
    "5. Identity negotiation (how they redefine or maintain their professional self)\n",
    "\n",
    "First-order codes should be close to the informant's own words and expressions.\n",
    "Identify 5-15 relevant codes from this interview.\n",
    "\n",
    "Respond with a JSON array of strings:\n",
    "[\"code 1\", \"code 2\", \"code 3\", ...]\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, list) else []\n",
    "    \n",
    "    def second_order_coding(self, first_order_codes: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Codage de second ordre - th√®mes √©mergents.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{self.research_question}\n",
    "\n",
    "FIRST-ORDER CODES from interviews with professionals about AI:\n",
    "{json.dumps(first_order_codes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Group these first-order codes into SECOND-ORDER THEMES.\n",
    "Second-order themes are more abstract, researcher-driven concepts that capture:\n",
    "- Identity maintenance strategies\n",
    "- Threat perception patterns\n",
    "- Legitimacy construction mechanisms\n",
    "- Adaptation and resistance behaviors\n",
    "- Professional boundary work\n",
    "\n",
    "Create 5-10 meaningful second-order themes.\n",
    "\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "  \"Theme Name\": [\"first-order code 1\", \"first-order code 2\"],\n",
    "  \"Another Theme\": [\"code 3\", \"code 4\", \"code 5\"]\n",
    "}}\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, dict) else {}\n",
    "    \n",
    "    def aggregate_dimensions(self, second_order_codes: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Dimensions agr√©g√©es - concepts th√©oriques.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{self.research_question}\n",
    "\n",
    "SECOND-ORDER THEMES:\n",
    "{json.dumps(second_order_codes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Aggregate these second-order themes into AGGREGATE DIMENSIONS.\n",
    "Aggregate dimensions are high-level theoretical constructs that:\n",
    "- Answer the research question\n",
    "- Connect to existing theory (identity theory, legitimacy theory, technology acceptance, etc.)\n",
    "- Provide a framework for understanding professional identity negotiation\n",
    "\n",
    "Consider dimensions related to:\n",
    "- Identity work (construction, maintenance, transformation)\n",
    "- Legitimacy strategies (claiming, defending, redefining)\n",
    "- Human-AI boundary negotiation\n",
    "- Professional resilience mechanisms\n",
    "\n",
    "Create 3-5 aggregate dimensions.\n",
    "\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "  \"Aggregate Dimension 1\": [\"Theme A\", \"Theme B\"],\n",
    "  \"Aggregate Dimension 2\": [\"Theme C\", \"Theme D\"]\n",
    "}}\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, dict) else {}\n",
    "    \n",
    "    def run_full_analysis(self, interviews: List[Dict]) -> Dict:\n",
    "        \"\"\"Ex√©cute l'analyse Gioia compl√®te.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üî¨ GIOIA ANALYSIS - Professional Identity & AI\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nüìã Research Question:\\n{self.research_question}\\n\")\n",
    "        \n",
    "        # √âtape 1: Codage initial\n",
    "        print(\"\\nüìù STEP 1: First-Order Coding...\")\n",
    "        all_first_order = []\n",
    "        coded_interviews = []\n",
    "        \n",
    "        for i, interview in enumerate(tqdm(interviews, desc=\"Coding interviews\")):\n",
    "            codes = self.initial_coding(interview)\n",
    "            interview['first_order_codes'] = codes\n",
    "            coded_interviews.append(interview)\n",
    "            all_first_order.extend(codes)\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   Processed {i+1}/{len(interviews)} interviews, {len(set(all_first_order))} unique codes\")\n",
    "        \n",
    "        # D√©dupliquer\n",
    "        unique_first_order = list(set(all_first_order))\n",
    "        print(f\"\\n   ‚úÖ {len(unique_first_order)} unique first-order codes identified\")\n",
    "        \n",
    "        # √âtape 2: Codage de second ordre\n",
    "        print(\"\\nüìù STEP 2: Second-Order Coding...\")\n",
    "        second_order = self.second_order_coding(unique_first_order)\n",
    "        print(f\"   ‚úÖ {len(second_order)} second-order themes created\")\n",
    "        \n",
    "        # √âtape 3: Dimensions agr√©g√©es\n",
    "        print(\"\\nüìù STEP 3: Aggregate Dimensions...\")\n",
    "        dimensions = self.aggregate_dimensions(second_order)\n",
    "        print(f\"   ‚úÖ {len(dimensions)} aggregate dimensions created\")\n",
    "        \n",
    "        return {\n",
    "            'interviews': coded_interviews,\n",
    "            'first_order_codes': unique_first_order,\n",
    "            'second_order_themes': second_order,\n",
    "            'aggregate_dimensions': dimensions\n",
    "        }\n",
    "\n",
    "# Instance\n",
    "gioia = GioiaAnalysis(gemini, RESEARCH_QUESTION)\n",
    "print(\"‚úÖ GioiaAnalysis initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lancer l'Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cuter l'analyse Gioia compl√®te\n",
    "results = gioia.run_full_analysis(interviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def display_gioia_structure(results: Dict):\n",
    "    \"\"\"Affiche la structure Gioia compl√®te.\"\"\"\n",
    "    dimensions = results.get('aggregate_dimensions', {})\n",
    "    second_order = results.get('second_order_themes', {})\n",
    "    \n",
    "    html = \"<style>\"\n",
    "    html += \".gioia-table { border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; }\"\n",
    "    html += \".gioia-table th, .gioia-table td { border: 1px solid #ddd; padding: 10px; text-align: left; vertical-align: top; }\"\n",
    "    html += \".gioia-table th { background-color: #4a90d9; color: white; }\"\n",
    "    html += \".dim-cell { background-color: #e8f4e8; font-weight: bold; font-size: 14px; }\"\n",
    "    html += \".theme-cell { background-color: #fff8e8; }\"\n",
    "    html += \".code-cell { font-size: 12px; color: #555; }\"\n",
    "    html += \"</style>\"\n",
    "    \n",
    "    html += \"<h2>üìä Gioia Data Structure</h2>\"\n",
    "    html += \"<table class='gioia-table'>\"\n",
    "    html += \"<tr><th>1st Order Codes</th><th>2nd Order Themes</th><th>Aggregate Dimensions</th></tr>\"\n",
    "    \n",
    "    for dim_name, themes in dimensions.items():\n",
    "        first_row_dim = True\n",
    "        theme_count = len(themes)\n",
    "        \n",
    "        for theme in themes:\n",
    "            first_codes = second_order.get(theme, [])\n",
    "            codes_html = \"<br>\".join([f\"‚Ä¢ {c}\" for c in first_codes[:6]])\n",
    "            if len(first_codes) > 6:\n",
    "                codes_html += f\"<br><i>... +{len(first_codes)-6} more</i>\"\n",
    "            \n",
    "            html += \"<tr>\"\n",
    "            html += f\"<td class='code-cell'>{codes_html}</td>\"\n",
    "            html += f\"<td class='theme-cell'>{theme}</td>\"\n",
    "            \n",
    "            if first_row_dim:\n",
    "                html += f\"<td class='dim-cell' rowspan='{theme_count}'>{dim_name}</td>\"\n",
    "                first_row_dim = False\n",
    "            \n",
    "            html += \"</tr>\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Afficher la structure\n",
    "display_gioia_structure(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les statistiques\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Interviews analyzed: {len(results['interviews'])}\")\n",
    "print(f\"üìù First-order codes: {len(results['first_order_codes'])}\")\n",
    "print(f\"üè∑Ô∏è Second-order themes: {len(results['second_order_themes'])}\")\n",
    "print(f\"üì¶ Aggregate dimensions: {len(results['aggregate_dimensions'])}\")\n",
    "\n",
    "print(\"\\n\\nüì¶ AGGREGATE DIMENSIONS:\")\n",
    "print(\"-\"*40)\n",
    "for dim, themes in results['aggregate_dimensions'].items():\n",
    "    print(f\"\\nüîπ {dim}\")\n",
    "    for theme in themes:\n",
    "        code_count = len(results['second_order_themes'].get(theme, []))\n",
    "        print(f\"   ‚îî‚îÄ {theme} ({code_count} codes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Construction du Mod√®le Th√©orique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_theoretical_model(results: Dict, research_question: str) -> Dict:\n",
    "    \"\"\"Construit un mod√®le th√©orique √† partir des r√©sultats Gioia.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a qualitative research expert building grounded theory.\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{research_question}\n",
    "\n",
    "GIOIA ANALYSIS RESULTS:\n",
    "\n",
    "Aggregate Dimensions:\n",
    "{json.dumps(results['aggregate_dimensions'], ensure_ascii=False, indent=2)}\n",
    "\n",
    "Second-Order Themes:\n",
    "{json.dumps(results['second_order_themes'], ensure_ascii=False, indent=2)}\n",
    "\n",
    "Based on this analysis, construct a THEORETICAL MODEL that:\n",
    "\n",
    "1. Answers the research question\n",
    "2. Identifies key constructs and their relationships\n",
    "3. Proposes testable propositions\n",
    "4. Connects to existing literature on:\n",
    "   - Professional identity theory\n",
    "   - Legitimacy theory\n",
    "   - Technology acceptance/resistance\n",
    "   - Boundary work in professions\n",
    "\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "  \"model_name\": \"Name of your theoretical model\",\n",
    "  \"core_argument\": \"The central theoretical argument (2-3 sentences)\",\n",
    "  \"constructs\": [\n",
    "    {{\n",
    "      \"name\": \"Construct name\",\n",
    "      \"definition\": \"Definition\",\n",
    "      \"type\": \"independent/dependent/mediator/moderator/process\",\n",
    "      \"grounded_in\": [\"Aggregate dimension(s) it comes from\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"propositions\": [\n",
    "    \"P1: First proposition...\",\n",
    "    \"P2: Second proposition...\"\n",
    "  ],\n",
    "  \"theoretical_contributions\": [\n",
    "    \"Contribution 1\",\n",
    "    \"Contribution 2\"\n",
    "  ],\n",
    "  \"practical_implications\": [\n",
    "    \"Implication 1\",\n",
    "    \"Implication 2\"\n",
    "  ],\n",
    "  \"boundary_conditions\": [\n",
    "    \"When/where this model applies\"\n",
    "  ],\n",
    "  \"future_research\": [\n",
    "    \"Direction 1\",\n",
    "    \"Direction 2\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    model = gemini.generate_json(prompt)\n",
    "    return model if isinstance(model, dict) else {}\n",
    "\n",
    "# Construire le mod√®le\n",
    "print(\"üèóÔ∏è Building theoretical model...\")\n",
    "theoretical_model = build_theoretical_model(results, RESEARCH_QUESTION)\n",
    "print(\"‚úÖ Model constructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le mod√®le th√©orique\n",
    "def display_theoretical_model(model: Dict):\n",
    "    md = f\"\"\"# üß† {model.get('model_name', 'Theoretical Model')}\n",
    "\n",
    "## Core Argument\n",
    "{model.get('core_argument', 'N/A')}\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Constructs\n",
    "\"\"\"\n",
    "    for c in model.get('constructs', []):\n",
    "        md += f\"\\n### {c.get('name')} *({c.get('type')})*\\n\"\n",
    "        md += f\"{c.get('definition')}\\n\"\n",
    "        md += f\"- *Grounded in: {', '.join(c.get('grounded_in', []))}*\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Propositions\\n\"\n",
    "    for p in model.get('propositions', []):\n",
    "        md += f\"- **{p}**\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Theoretical Contributions\\n\"\n",
    "    for c in model.get('theoretical_contributions', []):\n",
    "        md += f\"- {c}\\n\"\n",
    "    \n",
    "    md += \"\\n## Practical Implications\\n\"\n",
    "    for i in model.get('practical_implications', []):\n",
    "        md += f\"- {i}\\n\"\n",
    "    \n",
    "    md += \"\\n## Boundary Conditions\\n\"\n",
    "    for b in model.get('boundary_conditions', []):\n",
    "        md += f\"- {b}\\n\"\n",
    "    \n",
    "    md += \"\\n## Future Research Directions\\n\"\n",
    "    for f in model.get('future_research', []):\n",
    "        md += f\"- {f}\\n\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "\n",
    "display_theoretical_model(theoretical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisation du Mod√®le (Mermaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_diagram(model: Dict) -> str:\n",
    "    \"\"\"G√©n√®re un diagramme Mermaid du mod√®le.\"\"\"\n",
    "    prompt = f\"\"\"Create a Mermaid flowchart diagram for this theoretical model:\n",
    "\n",
    "{json.dumps(model, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Requirements:\n",
    "- Use graph LR (left to right) or graph TD (top down)\n",
    "- Show all constructs as nodes\n",
    "- Show relationships based on propositions as labeled arrows\n",
    "- Use different node shapes for different construct types:\n",
    "  - Independent variables: rectangles\n",
    "  - Dependent variables: rounded rectangles\n",
    "  - Mediators: circles\n",
    "  - Moderators: diamonds\n",
    "- Add colors using style definitions\n",
    "\n",
    "Return ONLY the Mermaid code, no markdown markers.\n",
    "\"\"\"\n",
    "    \n",
    "    response = gemini.generate(prompt, temperature=0.3)\n",
    "    \n",
    "    # Nettoyer\n",
    "    cleaned = response.strip()\n",
    "    if cleaned.startswith('```mermaid'):\n",
    "        cleaned = cleaned[10:]\n",
    "    if cleaned.startswith('```'):\n",
    "        cleaned = cleaned[3:]\n",
    "    if cleaned.endswith('```'):\n",
    "        cleaned = cleaned[:-3]\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "# G√©n√©rer le diagramme\n",
    "print(\"üìä Generating model diagram...\")\n",
    "mermaid_code = generate_model_diagram(theoretical_model)\n",
    "print(\"\\nüìà Mermaid Diagram Code:\")\n",
    "print(\"-\"*40)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le diagramme (fonctionne dans Colab/Jupyter)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "html = f\"\"\"\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
    "<script>mermaid.initialize({{startOnLoad:true, theme:'default'}});</script>\n",
    "<div class=\"mermaid\">\n",
    "{mermaid_code}\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Compiler tous les r√©sultats\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'research_question': RESEARCH_QUESTION,\n",
    "        'dataset': 'Anthropic/AnthropicInterviewer',\n",
    "        'split': 'workforce',\n",
    "        'sample_size': len(interviews),\n",
    "        'model': MODEL_NAME,\n",
    "        'analysis_date': datetime.now().isoformat()\n",
    "    },\n",
    "    'gioia_analysis': {\n",
    "        'first_order_codes': results['first_order_codes'],\n",
    "        'second_order_themes': results['second_order_themes'],\n",
    "        'aggregate_dimensions': results['aggregate_dimensions']\n",
    "    },\n",
    "    'theoretical_model': theoretical_model,\n",
    "    'visualization': {\n",
    "        'mermaid_diagram': mermaid_code\n",
    "    },\n",
    "    'coded_interviews': [\n",
    "        {\n",
    "            'id': i['id'],\n",
    "            'first_order_codes': i.get('first_order_codes', [])\n",
    "        } for i in results['interviews']\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "filename = f\"grounded_theory_professional_identity_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {filename}\")\n",
    "\n",
    "# T√©l√©charger (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    print(\"üì• Download started...\")\n",
    "except:\n",
    "    print(\"(Manual download required outside Colab)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV des codes pour analyse suppl√©mentaire\n",
    "codes_data = []\n",
    "for theme, codes in results['second_order_themes'].items():\n",
    "    # Trouver la dimension agr√©g√©e\n",
    "    dimension = None\n",
    "    for dim, themes in results['aggregate_dimensions'].items():\n",
    "        if theme in themes:\n",
    "            dimension = dim\n",
    "            break\n",
    "    \n",
    "    for code in codes:\n",
    "        codes_data.append({\n",
    "            'first_order_code': code,\n",
    "            'second_order_theme': theme,\n",
    "            'aggregate_dimension': dimension\n",
    "        })\n",
    "\n",
    "df_codes = pd.DataFrame(codes_data)\n",
    "csv_filename = f\"gioia_codes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_codes.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n‚úÖ Codes exported to: {csv_filename}\")\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "display(df_codes.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ Notes\n",
    "\n",
    "### M√©thode Gioia\n",
    "La m√©thode Gioia est une approche rigoureuse pour d√©velopper une th√©orie ancr√©e:\n",
    "1. **Codes de 1er ordre**: Proches des mots des participants\n",
    "2. **Th√®mes de 2nd ordre**: Concepts plus abstraits du chercheur\n",
    "3. **Dimensions agr√©g√©es**: Construits th√©oriques de haut niveau\n",
    "\n",
    "### Personnalisation\n",
    "- Modifiez `SAMPLE_SIZE` pour analyser plus/moins d'interviews\n",
    "- Adaptez `RESEARCH_QUESTION` pour changer le focus\n",
    "- Le mod√®le peut √™tre chang√© dans la configuration\n",
    "\n",
    "### R√©f√©rences\n",
    "- Gioia, D. A., Corley, K. G., & Hamilton, A. L. (2013). Seeking qualitative rigor in inductive research\n",
    "- Dataset: [Anthropic/AnthropicInterviewer](https://huggingface.co/datasets/Anthropic/AnthropicInterviewer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
