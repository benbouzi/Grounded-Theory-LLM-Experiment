{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Theory Analysis: Professional Identity & AI\n",
    "\n",
    "## Research Question\n",
    "**\"How do professionals negotiate maintaining their identity and legitimacy in the face of a tool that threatens to replace them?\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset\n",
    "- **Source**: [Anthropic/AnthropicInterviewer](https://huggingface.co/datasets/Anthropic/AnthropicInterviewer)\n",
    "- **Split**: workforce (1,000 interviews)\n",
    "- **Method**: Gioia Methodology for Grounded Theory\n",
    "- **Model**: Gemini 2.5 Flash\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q google-generativeai\n!pip install -q datasets\n!pip install -q pandas\n!pip install -q tqdm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import google.generativeai as genai\nfrom datasets import load_dataset\nimport json\nimport re\nfrom typing import List, Dict, Any\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport time\nimport random\n\n# ============================================\n# CONFIGURATION\n# ============================================\n\n# Cl√© API Gemini - UTILISER LES SECRETS COLAB\nfrom google.colab import userdata\nGOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n\n# Mod√®le Gemini\nMODEL_NAME = \"gemini-2.5-flash\"\n\n# Question de recherche\nRESEARCH_QUESTION = \"\"\"How do professionals negotiate maintaining their identity and legitimacy \nin the face of a tool that threatens to replace them?\"\"\"\n\n# Nombre d'interviews √† analyser\nSAMPLE_SIZE = 50  # Mettre None pour tout analyser (1000)\n\n# Traitement: 1 interview √† la fois (plus fiable pour interviews longues)\nBATCH_SIZE = 1\n\n# Limite de caract√®res par interview (pour √©viter erreur 400)\nMAX_CHARS_PER_INTERVIEW = 8000\n\n# Configuration API\ngenai.configure(api_key=GOOGLE_API_KEY)\n\nprint(f\"‚úÖ Configuration charg√©e\")\nprint(f\"   Mod√®le: {MODEL_NAME}\")\nprint(f\"   Sample: {SAMPLE_SIZE or 'Tous (1000)'}\")\nprint(f\"   Max chars/interview: {MAX_CHARS_PER_INTERVIEW}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset depuis Hugging Face\n",
    "print(\"üì• Chargement du dataset Anthropic/AnthropicInterviewer...\")\n",
    "dataset = load_dataset(\"Anthropic/AnthropicInterviewer\", split=\"workforce\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset charg√©: {len(dataset)} interviews\")\n",
    "print(f\"\\nüìã Colonnes disponibles: {dataset.column_names}\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(\"\\nüìÑ Exemple d'interview:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pr√©parer les donn√©es pour l'analyse\ndef prepare_interviews(dataset, sample_size=None):\n    \"\"\"Pr√©pare les interviews pour l'analyse (simplifi√© pour interviews courtes).\"\"\"\n    interviews = []\n    \n    # √âchantillonner si n√©cessaire\n    indices = list(range(len(dataset)))\n    if sample_size and sample_size < len(dataset):\n        indices = random.sample(indices, sample_size)\n    \n    for idx in indices:\n        item = dataset[idx]\n        \n        interview = {\n            'id': idx,\n            'content': '',\n        }\n        \n        # Extraire le contenu textuel selon la structure du dataset\n        if isinstance(item, dict):\n            # Chercher les champs de texte courants\n            for key in ['text', 'content', 'transcript', 'conversation', 'messages']:\n                if key in item and item[key]:\n                    val = item[key]\n                    if isinstance(val, str):\n                        interview['content'] = val\n                    elif isinstance(val, list):\n                        # Liste de messages -> joindre\n                        interview['content'] = \"\\n\".join([\n                            f\"{m.get('role', 'unknown')}: {m.get('content', str(m))}\" \n                            if isinstance(m, dict) else str(m) \n                            for m in val\n                        ])\n                    break\n            \n            # Fallback: utiliser tout le dict\n            if not interview['content']:\n                interview['content'] = json.dumps(item, ensure_ascii=False)\n        else:\n            interview['content'] = str(item)\n        \n        interviews.append(interview)\n    \n    return interviews\n\n# Pr√©parer les interviews\ninterviews = prepare_interviews(dataset, SAMPLE_SIZE)\n\nprint(f\"‚úÖ {len(interviews)} interviews pr√©par√©es\")\nprint(f\"\\nüìè Longueur moyenne: {sum(len(i['content']) for i in interviews) // len(interviews)} caract√®res\")\nprint(f\"\\nüìÑ Aper√ßu interview #1 (500 chars):\")\nprint(\"-\"*40)\nprint(interviews[0]['content'][:500] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Service Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiService:\n",
    "    \"\"\"Service pour les appels √† l'API Gemini avec gestion des erreurs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME):\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Gestion du rate limiting.\"\"\"\n",
    "        self.request_count += 1\n",
    "        # Pause toutes les 10 requ√™tes pour √©viter les limites\n",
    "        if self.request_count % 10 == 0:\n",
    "            time.sleep(2)\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.7, max_retries: int = 3) -> str:\n",
    "        \"\"\"G√©n√®re une r√©ponse avec retry automatique.\"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        config = genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            max_output_tokens=8192,\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt, generation_config=config)\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = (attempt + 1) * 5\n",
    "                    print(f\"‚ö†Ô∏è Erreur API, retry dans {wait_time}s: {str(e)[:100]}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå √âchec apr√®s {max_retries} tentatives: {e}\")\n",
    "                    return \"\"\n",
    "    \n",
    "    def generate_json(self, prompt: str, temperature: float = 0.3) -> Any:\n",
    "        \"\"\"G√©n√®re une r√©ponse JSON structur√©e.\"\"\"\n",
    "        json_prompt = f\"\"\"{prompt}\n",
    "\n",
    "CRITICAL: Respond ONLY with valid JSON. No text before or after.\n",
    "Do not use ```json or ``` markers.\n",
    "\"\"\"\n",
    "        response = self.generate(json_prompt, temperature=temperature)\n",
    "        \n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        # Nettoyer la r√©ponse\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith('```json'):\n",
    "            cleaned = cleaned[7:]\n",
    "        if cleaned.startswith('```'):\n",
    "            cleaned = cleaned[3:]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned.strip())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è JSON parsing error: {e}\")\n",
    "            # Essayer d'extraire le JSON\n",
    "            match = re.search(r'[\\[\\{].*[\\]\\}]', cleaned, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group())\n",
    "                except:\n",
    "                    pass\n",
    "            return None\n",
    "\n",
    "# Instance globale\n",
    "gemini = GeminiService()\n",
    "print(\"‚úÖ GeminiService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Codage Gioia - Adapt√© √† la Question de Recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GioiaAnalysis:\n    \"\"\"Analyse Gioia optimis√©e - traite une interview √† la fois pour plus de fiabilit√©.\"\"\"\n    \n    def __init__(self, gemini_service: GeminiService, research_question: str):\n        self.gemini = gemini_service\n        self.research_question = research_question\n    \n    def clean_text(self, text: str) -> str:\n        \"\"\"Nettoie le texte pour √©viter les erreurs API.\"\"\"\n        # Supprimer les caract√®res de contr√¥le probl√©matiques\n        text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n        # Limiter la longueur\n        return text[:MAX_CHARS_PER_INTERVIEW]\n    \n    def code_single_interview(self, interview: Dict) -> List[str]:\n        \"\"\"Code une seule interview.\"\"\"\n        content = self.clean_text(interview['content'])\n        \n        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n\nRESEARCH QUESTION: {self.research_question}\n\nINTERVIEW TRANSCRIPT:\n{content}\n\nAnalyze this interview and identify FIRST-ORDER CODES (5-12 codes).\nFocus on:\n- Professional identity and self-definition\n- Perceived threats from AI\n- Coping and adaptation strategies\n- Legitimacy claims and value justification\n- Identity negotiation with technology\n\nFirst-order codes should be close to the informant's own words.\n\nRespond ONLY with a JSON array of strings:\n[\"code 1\", \"code 2\", \"code 3\"]\n\"\"\"\n        result = self.gemini.generate_json(prompt)\n        return result if isinstance(result, list) else []\n    \n    def initial_coding(self, interviews: List[Dict]) -> tuple:\n        \"\"\"Codage de premier ordre - une interview √† la fois.\"\"\"\n        coded_interviews = []\n        all_codes = []\n        \n        for interview in tqdm(interviews, desc=\"Coding interviews\"):\n            codes = self.code_single_interview(interview)\n            interview['first_order_codes'] = codes\n            coded_interviews.append(interview)\n            all_codes.extend(codes)\n            \n            # Petite pause pour √©viter rate limiting\n            time.sleep(0.5)\n        \n        return coded_interviews, list(set(all_codes))\n    \n    def second_order_coding(self, first_order_codes: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Codage de second ordre - th√®mes √©mergents.\"\"\"\n        # Limiter le nombre de codes envoy√©s\n        codes_to_send = first_order_codes[:150]\n        \n        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n\nRESEARCH QUESTION: {self.research_question}\n\nFIRST-ORDER CODES ({len(codes_to_send)} codes from professional interviews about AI):\n{json.dumps(codes_to_send, ensure_ascii=False, indent=2)}\n\nGroup these into 6-10 SECOND-ORDER THEMES.\nSecond-order themes are more abstract concepts capturing:\n- Identity maintenance strategies\n- Threat perception patterns\n- Legitimacy construction mechanisms\n- Adaptation behaviors\n- Professional boundary work\n\nRespond with a JSON object:\n{{\"Theme Name\": [\"code1\", \"code2\"], \"Another Theme\": [\"code3\", \"code4\"]}}\n\"\"\"\n        result = self.gemini.generate_json(prompt)\n        return result if isinstance(result, dict) else {}\n    \n    def aggregate_dimensions(self, second_order_codes: Dict[str, List[str]]) -> Dict[str, List[str]]:\n        \"\"\"Dimensions agr√©g√©es - concepts th√©oriques.\"\"\"\n        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n\nRESEARCH QUESTION: {self.research_question}\n\nSECOND-ORDER THEMES:\n{json.dumps(second_order_codes, ensure_ascii=False, indent=2)}\n\nAggregate these into 3-5 AGGREGATE DIMENSIONS.\nAggregate dimensions are high-level theoretical constructs that answer the research question.\n\nRespond with a JSON object:\n{{\"Dimension Name\": [\"Theme A\", \"Theme B\"], \"Another Dimension\": [\"Theme C\"]}}\n\"\"\"\n        result = self.gemini.generate_json(prompt)\n        return result if isinstance(result, dict) else {}\n    \n    def run_full_analysis(self, interviews: List[Dict]) -> Dict:\n        \"\"\"Ex√©cute l'analyse Gioia compl√®te.\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(\"üî¨ GIOIA ANALYSIS - Professional Identity & AI\")\n        print(f\"{'='*60}\")\n        print(f\"\\nüìã Research Question:\\n{self.research_question}\\n\")\n        print(f\"üìä {len(interviews)} interviews to analyze\")\n        \n        # √âtape 1: Codage initial\n        print(\"\\nüìù STEP 1: First-Order Coding...\")\n        coded_interviews, unique_first_order = self.initial_coding(interviews)\n        print(f\"   ‚úÖ {len(unique_first_order)} unique first-order codes identified\")\n        \n        # √âtape 2: Codage de second ordre\n        print(\"\\nüìù STEP 2: Second-Order Coding...\")\n        second_order = self.second_order_coding(unique_first_order)\n        print(f\"   ‚úÖ {len(second_order)} second-order themes created\")\n        \n        # √âtape 3: Dimensions agr√©g√©es\n        print(\"\\nüìù STEP 3: Aggregate Dimensions...\")\n        dimensions = self.aggregate_dimensions(second_order)\n        print(f\"   ‚úÖ {len(dimensions)} aggregate dimensions created\")\n        \n        return {\n            'interviews': coded_interviews,\n            'first_order_codes': unique_first_order,\n            'second_order_themes': second_order,\n            'aggregate_dimensions': dimensions\n        }\n\n# Instance\ngioia = GioiaAnalysis(gemini, RESEARCH_QUESTION)\nprint(\"‚úÖ GioiaAnalysis initialis√©\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lancer l'Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ex√©cuter l'analyse Gioia compl√®te\nresults = gioia.run_full_analysis(interviews)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def display_gioia_structure(results: Dict):\n",
    "    \"\"\"Affiche la structure Gioia compl√®te.\"\"\"\n",
    "    dimensions = results.get('aggregate_dimensions', {})\n",
    "    second_order = results.get('second_order_themes', {})\n",
    "    \n",
    "    html = \"<style>\"\n",
    "    html += \".gioia-table { border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; }\"\n",
    "    html += \".gioia-table th, .gioia-table td { border: 1px solid #ddd; padding: 10px; text-align: left; vertical-align: top; }\"\n",
    "    html += \".gioia-table th { background-color: #4a90d9; color: white; }\"\n",
    "    html += \".dim-cell { background-color: #e8f4e8; font-weight: bold; font-size: 14px; }\"\n",
    "    html += \".theme-cell { background-color: #fff8e8; }\"\n",
    "    html += \".code-cell { font-size: 12px; color: #555; }\"\n",
    "    html += \"</style>\"\n",
    "    \n",
    "    html += \"<h2>üìä Gioia Data Structure</h2>\"\n",
    "    html += \"<table class='gioia-table'>\"\n",
    "    html += \"<tr><th>1st Order Codes</th><th>2nd Order Themes</th><th>Aggregate Dimensions</th></tr>\"\n",
    "    \n",
    "    for dim_name, themes in dimensions.items():\n",
    "        first_row_dim = True\n",
    "        theme_count = len(themes)\n",
    "        \n",
    "        for theme in themes:\n",
    "            first_codes = second_order.get(theme, [])\n",
    "            codes_html = \"<br>\".join([f\"‚Ä¢ {c}\" for c in first_codes[:6]])\n",
    "            if len(first_codes) > 6:\n",
    "                codes_html += f\"<br><i>... +{len(first_codes)-6} more</i>\"\n",
    "            \n",
    "            html += \"<tr>\"\n",
    "            html += f\"<td class='code-cell'>{codes_html}</td>\"\n",
    "            html += f\"<td class='theme-cell'>{theme}</td>\"\n",
    "            \n",
    "            if first_row_dim:\n",
    "                html += f\"<td class='dim-cell' rowspan='{theme_count}'>{dim_name}</td>\"\n",
    "                first_row_dim = False\n",
    "            \n",
    "            html += \"</tr>\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Afficher la structure\n",
    "display_gioia_structure(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les statistiques\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Interviews analyzed: {len(results['interviews'])}\")\n",
    "print(f\"üìù First-order codes: {len(results['first_order_codes'])}\")\n",
    "print(f\"üè∑Ô∏è Second-order themes: {len(results['second_order_themes'])}\")\n",
    "print(f\"üì¶ Aggregate dimensions: {len(results['aggregate_dimensions'])}\")\n",
    "\n",
    "print(\"\\n\\nüì¶ AGGREGATE DIMENSIONS:\")\n",
    "print(\"-\"*40)\n",
    "for dim, themes in results['aggregate_dimensions'].items():\n",
    "    print(f\"\\nüîπ {dim}\")\n",
    "    for theme in themes:\n",
    "        code_count = len(results['second_order_themes'].get(theme, []))\n",
    "        print(f\"   ‚îî‚îÄ {theme} ({code_count} codes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Construction du Mod√®le Th√©orique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_theoretical_model(results: Dict, research_question: str) -> Dict:\n",
    "    \"\"\"Construit un mod√®le th√©orique √† partir des r√©sultats Gioia.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a qualitative research expert building grounded theory.\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{research_question}\n",
    "\n",
    "GIOIA ANALYSIS RESULTS:\n",
    "\n",
    "Aggregate Dimensions:\n",
    "{json.dumps(results['aggregate_dimensions'], ensure_ascii=False, indent=2)}\n",
    "\n",
    "Second-Order Themes:\n",
    "{json.dumps(results['second_order_themes'], ensure_ascii=False, indent=2)}\n",
    "\n",
    "Based on this analysis, construct a THEORETICAL MODEL that:\n",
    "\n",
    "1. Answers the research question\n",
    "2. Identifies key constructs and their relationships\n",
    "3. Proposes testable propositions\n",
    "4. Connects to existing literature on:\n",
    "   - Professional identity theory\n",
    "   - Legitimacy theory\n",
    "   - Technology acceptance/resistance\n",
    "   - Boundary work in professions\n",
    "\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "  \"model_name\": \"Name of your theoretical model\",\n",
    "  \"core_argument\": \"The central theoretical argument (2-3 sentences)\",\n",
    "  \"constructs\": [\n",
    "    {{\n",
    "      \"name\": \"Construct name\",\n",
    "      \"definition\": \"Definition\",\n",
    "      \"type\": \"independent/dependent/mediator/moderator/process\",\n",
    "      \"grounded_in\": [\"Aggregate dimension(s) it comes from\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"propositions\": [\n",
    "    \"P1: First proposition...\",\n",
    "    \"P2: Second proposition...\"\n",
    "  ],\n",
    "  \"theoretical_contributions\": [\n",
    "    \"Contribution 1\",\n",
    "    \"Contribution 2\"\n",
    "  ],\n",
    "  \"practical_implications\": [\n",
    "    \"Implication 1\",\n",
    "    \"Implication 2\"\n",
    "  ],\n",
    "  \"boundary_conditions\": [\n",
    "    \"When/where this model applies\"\n",
    "  ],\n",
    "  \"future_research\": [\n",
    "    \"Direction 1\",\n",
    "    \"Direction 2\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    model = gemini.generate_json(prompt)\n",
    "    return model if isinstance(model, dict) else {}\n",
    "\n",
    "# Construire le mod√®le\n",
    "print(\"üèóÔ∏è Building theoretical model...\")\n",
    "theoretical_model = build_theoretical_model(results, RESEARCH_QUESTION)\n",
    "print(\"‚úÖ Model constructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le mod√®le th√©orique\n",
    "def display_theoretical_model(model: Dict):\n",
    "    md = f\"\"\"# üß† {model.get('model_name', 'Theoretical Model')}\n",
    "\n",
    "## Core Argument\n",
    "{model.get('core_argument', 'N/A')}\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Constructs\n",
    "\"\"\"\n",
    "    for c in model.get('constructs', []):\n",
    "        md += f\"\\n### {c.get('name')} *({c.get('type')})*\\n\"\n",
    "        md += f\"{c.get('definition')}\\n\"\n",
    "        md += f\"- *Grounded in: {', '.join(c.get('grounded_in', []))}*\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Propositions\\n\"\n",
    "    for p in model.get('propositions', []):\n",
    "        md += f\"- **{p}**\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Theoretical Contributions\\n\"\n",
    "    for c in model.get('theoretical_contributions', []):\n",
    "        md += f\"- {c}\\n\"\n",
    "    \n",
    "    md += \"\\n## Practical Implications\\n\"\n",
    "    for i in model.get('practical_implications', []):\n",
    "        md += f\"- {i}\\n\"\n",
    "    \n",
    "    md += \"\\n## Boundary Conditions\\n\"\n",
    "    for b in model.get('boundary_conditions', []):\n",
    "        md += f\"- {b}\\n\"\n",
    "    \n",
    "    md += \"\\n## Future Research Directions\\n\"\n",
    "    for f in model.get('future_research', []):\n",
    "        md += f\"- {f}\\n\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "\n",
    "display_theoretical_model(theoretical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisation du Mod√®le (Mermaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_diagram(model: Dict) -> str:\n",
    "    \"\"\"G√©n√®re un diagramme Mermaid du mod√®le.\"\"\"\n",
    "    prompt = f\"\"\"Create a Mermaid flowchart diagram for this theoretical model:\n",
    "\n",
    "{json.dumps(model, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Requirements:\n",
    "- Use graph LR (left to right) or graph TD (top down)\n",
    "- Show all constructs as nodes\n",
    "- Show relationships based on propositions as labeled arrows\n",
    "- Use different node shapes for different construct types:\n",
    "  - Independent variables: rectangles\n",
    "  - Dependent variables: rounded rectangles\n",
    "  - Mediators: circles\n",
    "  - Moderators: diamonds\n",
    "- Add colors using style definitions\n",
    "\n",
    "Return ONLY the Mermaid code, no markdown markers.\n",
    "\"\"\"\n",
    "    \n",
    "    response = gemini.generate(prompt, temperature=0.3)\n",
    "    \n",
    "    # Nettoyer\n",
    "    cleaned = response.strip()\n",
    "    if cleaned.startswith('```mermaid'):\n",
    "        cleaned = cleaned[10:]\n",
    "    if cleaned.startswith('```'):\n",
    "        cleaned = cleaned[3:]\n",
    "    if cleaned.endswith('```'):\n",
    "        cleaned = cleaned[:-3]\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "# G√©n√©rer le diagramme\n",
    "print(\"üìä Generating model diagram...\")\n",
    "mermaid_code = generate_model_diagram(theoretical_model)\n",
    "print(\"\\nüìà Mermaid Diagram Code:\")\n",
    "print(\"-\"*40)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le diagramme (fonctionne dans Colab/Jupyter)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "html = f\"\"\"\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
    "<script>mermaid.initialize({{startOnLoad:true, theme:'default'}});</script>\n",
    "<div class=\"mermaid\">\n",
    "{mermaid_code}\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Compiler tous les r√©sultats\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'research_question': RESEARCH_QUESTION,\n",
    "        'dataset': 'Anthropic/AnthropicInterviewer',\n",
    "        'split': 'workforce',\n",
    "        'sample_size': len(interviews),\n",
    "        'model': MODEL_NAME,\n",
    "        'analysis_date': datetime.now().isoformat()\n",
    "    },\n",
    "    'gioia_analysis': {\n",
    "        'first_order_codes': results['first_order_codes'],\n",
    "        'second_order_themes': results['second_order_themes'],\n",
    "        'aggregate_dimensions': results['aggregate_dimensions']\n",
    "    },\n",
    "    'theoretical_model': theoretical_model,\n",
    "    'visualization': {\n",
    "        'mermaid_diagram': mermaid_code\n",
    "    },\n",
    "    'coded_interviews': [\n",
    "        {\n",
    "            'id': i['id'],\n",
    "            'first_order_codes': i.get('first_order_codes', [])\n",
    "        } for i in results['interviews']\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "filename = f\"grounded_theory_professional_identity_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {filename}\")\n",
    "\n",
    "# T√©l√©charger (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    print(\"üì• Download started...\")\n",
    "except:\n",
    "    print(\"(Manual download required outside Colab)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV des codes pour analyse suppl√©mentaire\n",
    "codes_data = []\n",
    "for theme, codes in results['second_order_themes'].items():\n",
    "    # Trouver la dimension agr√©g√©e\n",
    "    dimension = None\n",
    "    for dim, themes in results['aggregate_dimensions'].items():\n",
    "        if theme in themes:\n",
    "            dimension = dim\n",
    "            break\n",
    "    \n",
    "    for code in codes:\n",
    "        codes_data.append({\n",
    "            'first_order_code': code,\n",
    "            'second_order_theme': theme,\n",
    "            'aggregate_dimension': dimension\n",
    "        })\n",
    "\n",
    "df_codes = pd.DataFrame(codes_data)\n",
    "csv_filename = f\"gioia_codes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_codes.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n‚úÖ Codes exported to: {csv_filename}\")\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "display(df_codes.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ Notes\n",
    "\n",
    "### M√©thode Gioia\n",
    "La m√©thode Gioia est une approche rigoureuse pour d√©velopper une th√©orie ancr√©e:\n",
    "1. **Codes de 1er ordre**: Proches des mots des participants\n",
    "2. **Th√®mes de 2nd ordre**: Concepts plus abstraits du chercheur\n",
    "3. **Dimensions agr√©g√©es**: Construits th√©oriques de haut niveau\n",
    "\n",
    "### Personnalisation\n",
    "- Modifiez `SAMPLE_SIZE` pour analyser plus/moins d'interviews\n",
    "- Adaptez `RESEARCH_QUESTION` pour changer le focus\n",
    "- Le mod√®le peut √™tre chang√© dans la configuration\n",
    "\n",
    "### R√©f√©rences\n",
    "- Gioia, D. A., Corley, K. G., & Hamilton, A. L. (2013). Seeking qualitative rigor in inductive research\n",
    "- Dataset: [Anthropic/AnthropicInterviewer](https://huggingface.co/datasets/Anthropic/AnthropicInterviewer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}