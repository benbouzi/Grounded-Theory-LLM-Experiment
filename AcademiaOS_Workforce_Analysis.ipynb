{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Theory Analysis: Professional Identity & AI\n",
    "\n",
    "## Research Question\n",
    "**\"How do professionals negotiate maintaining their identity and legitimacy in the face of a tool that threatens to replace them?\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset\n",
    "- **Source**: [Anthropic/AnthropicInterviewer](https://huggingface.co/datasets/Anthropic/AnthropicInterviewer)\n",
    "- **Split**: workforce (1,000 interviews)\n",
    "- **Method**: Gioia Methodology for Grounded Theory\n",
    "- **Model**: Gemini 2.5 Flash\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q google-generativeai\n!pip install -q datasets\n!pip install -q pandas\n!pip install -q tqdm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import google.generativeai as genai\nfrom datasets import load_dataset\nimport json\nimport re\nfrom typing import List, Dict, Any\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport time\nimport random\n\n# ============================================\n# CONFIGURATION\n# ============================================\n\n# Cl√© API Gemini\nGOOGLE_API_KEY = \"AIzaSyDwJo__wvgb_FENhpG0bieFO03K-WAxGME\"\n\n# Mod√®le Gemini - Utiliser le nom correct\nMODEL_NAME = \"gemini-2.5-flash\"  # ou \"gemini-1.5-flash\" si 2.5 ne marche pas\n\n# Question de recherche\nRESEARCH_QUESTION = \"\"\"How do professionals negotiate maintaining their identity and legitimacy \nin the face of a tool that threatens to replace them?\"\"\"\n\n# Nombre d'interviews √† analyser\nSAMPLE_SIZE = 50  # Mettre None pour tout analyser (1000)\n\n# Batch size pour le codage initial (combien d'interviews par requ√™te)\nBATCH_SIZE = 5  # Traiter 5 interviews √† la fois = plus rapide\n\n# Configuration API\ngenai.configure(api_key=GOOGLE_API_KEY)\n\n# V√©rifier les mod√®les disponibles\nprint(\"üìã Mod√®les disponibles:\")\nfor m in genai.list_models():\n    if \"generateContent\" in m.supported_generation_methods:\n        print(f\"   - {m.name}\")\n\nprint(f\"\\n‚úÖ Configuration charg√©e\")\nprint(f\"   Mod√®le s√©lectionn√©: {MODEL_NAME}\")\nprint(f\"   Sample: {SAMPLE_SIZE or 'Tous (1000)'}\")\nprint(f\"   Batch size: {BATCH_SIZE} interviews/requ√™te\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset depuis Hugging Face\n",
    "print(\"üì• Chargement du dataset Anthropic/AnthropicInterviewer...\")\n",
    "dataset = load_dataset(\"Anthropic/AnthropicInterviewer\", split=\"workforce\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset charg√©: {len(dataset)} interviews\")\n",
    "print(f\"\\nüìã Colonnes disponibles: {dataset.column_names}\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(\"\\nüìÑ Exemple d'interview:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pr√©parer les donn√©es pour l'analyse\ndef prepare_interviews(dataset, sample_size=None):\n    \"\"\"Pr√©pare les interviews pour l'analyse (simplifi√© pour interviews courtes).\"\"\"\n    interviews = []\n    \n    # √âchantillonner si n√©cessaire\n    indices = list(range(len(dataset)))\n    if sample_size and sample_size < len(dataset):\n        indices = random.sample(indices, sample_size)\n    \n    for idx in indices:\n        item = dataset[idx]\n        \n        interview = {\n            'id': idx,\n            'content': '',\n        }\n        \n        # Extraire le contenu textuel selon la structure du dataset\n        if isinstance(item, dict):\n            # Chercher les champs de texte courants\n            for key in ['text', 'content', 'transcript', 'conversation', 'messages']:\n                if key in item and item[key]:\n                    val = item[key]\n                    if isinstance(val, str):\n                        interview['content'] = val\n                    elif isinstance(val, list):\n                        # Liste de messages -> joindre\n                        interview['content'] = \"\\n\".join([\n                            f\"{m.get('role', 'unknown')}: {m.get('content', str(m))}\" \n                            if isinstance(m, dict) else str(m) \n                            for m in val\n                        ])\n                    break\n            \n            # Fallback: utiliser tout le dict\n            if not interview['content']:\n                interview['content'] = json.dumps(item, ensure_ascii=False)\n        else:\n            interview['content'] = str(item)\n        \n        interviews.append(interview)\n    \n    return interviews\n\n# Pr√©parer les interviews\ninterviews = prepare_interviews(dataset, SAMPLE_SIZE)\n\nprint(f\"‚úÖ {len(interviews)} interviews pr√©par√©es\")\nprint(f\"\\nüìè Longueur moyenne: {sum(len(i['content']) for i in interviews) // len(interviews)} caract√®res\")\nprint(f\"\\nüìÑ Aper√ßu interview #1 (500 chars):\")\nprint(\"-\"*40)\nprint(interviews[0]['content'][:500] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Service Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiService:\n",
    "    \"\"\"Service pour les appels √† l'API Gemini avec gestion des erreurs.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME):\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = 0\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Gestion du rate limiting.\"\"\"\n",
    "        self.request_count += 1\n",
    "        # Pause toutes les 10 requ√™tes pour √©viter les limites\n",
    "        if self.request_count % 10 == 0:\n",
    "            time.sleep(2)\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.7, max_retries: int = 3) -> str:\n",
    "        \"\"\"G√©n√®re une r√©ponse avec retry automatique.\"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        config = genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            max_output_tokens=8192,\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt, generation_config=config)\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = (attempt + 1) * 5\n",
    "                    print(f\"‚ö†Ô∏è Erreur API, retry dans {wait_time}s: {str(e)[:100]}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå √âchec apr√®s {max_retries} tentatives: {e}\")\n",
    "                    return \"\"\n",
    "    \n",
    "    def generate_json(self, prompt: str, temperature: float = 0.3) -> Any:\n",
    "        \"\"\"G√©n√®re une r√©ponse JSON structur√©e.\"\"\"\n",
    "        json_prompt = f\"\"\"{prompt}\n",
    "\n",
    "CRITICAL: Respond ONLY with valid JSON. No text before or after.\n",
    "Do not use ```json or ``` markers.\n",
    "\"\"\"\n",
    "        response = self.generate(json_prompt, temperature=temperature)\n",
    "        \n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        # Nettoyer la r√©ponse\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith('```json'):\n",
    "            cleaned = cleaned[7:]\n",
    "        if cleaned.startswith('```'):\n",
    "            cleaned = cleaned[3:]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned.strip())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è JSON parsing error: {e}\")\n",
    "            # Essayer d'extraire le JSON\n",
    "            match = re.search(r'[\\[\\{].*[\\]\\}]', cleaned, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group())\n",
    "                except:\n",
    "                    pass\n",
    "            return None\n",
    "\n",
    "# Instance globale\n",
    "gemini = GeminiService()\n",
    "print(\"‚úÖ GeminiService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Codage Gioia - Adapt√© √† la Question de Recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GioiaAnalysis:\n    \"\"\"Analyse Gioia optimis√©e pour les interviews courtes (batch processing).\"\"\"\n    \n    def __init__(self, gemini_service: GeminiService, research_question: str):\n        self.gemini = gemini_service\n        self.research_question = research_question\n    \n    def batch_initial_coding(self, interviews: List[Dict], batch_size: int = 5) -> List[Dict]:\n        \"\"\"Codage de premier ordre par batch (plus efficace pour interviews courtes).\"\"\"\n        coded_interviews = []\n        all_codes = []\n        \n        # Traiter par batch\n        for i in range(0, len(interviews), batch_size):\n            batch = interviews[i:i+batch_size]\n            \n            # Pr√©parer le texte des interviews du batch\n            interviews_text = \"\"\n            for j, interview in enumerate(batch):\n                interviews_text += f\"\\n--- INTERVIEW {i+j+1} ---\\n\"\n                interviews_text += interview['content'][:3000]  # Limiter chaque interview\n                interviews_text += \"\\n\"\n            \n            prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n\nRESEARCH QUESTION:\n{self.research_question}\n\nAnalyze the following {len(batch)} interview transcripts and identify FIRST-ORDER CODES for EACH interview.\n\nFocus on codes related to:\n1. Professional identity (how they define themselves, their expertise, their role)\n2. Perceived threats from AI (fears, concerns, anxieties about replacement)  \n3. Coping strategies (how they adapt, resist, or embrace AI)\n4. Legitimacy claims (how they justify their continued value/relevance)\n5. Identity negotiation (how they redefine or maintain their professional self)\n\nFirst-order codes should be close to the informant's own words.\nIdentify 3-10 codes per interview.\n\n{interviews_text}\n\nRespond with a JSON object where keys are interview numbers:\n{{\n  \"interview_1\": [\"code 1\", \"code 2\", \"code 3\"],\n  \"interview_2\": [\"code 1\", \"code 2\"],\n  ...\n}}\n\"\"\"\n            result = self.gemini.generate_json(prompt)\n            \n            if result and isinstance(result, dict):\n                for j, interview in enumerate(batch):\n                    key = f\"interview_{j+1}\"\n                    codes = result.get(key, [])\n                    if not codes:\n                        # Essayer d'autres formats de cl√©\n                        codes = result.get(str(j+1), result.get(f\"{i+j+1}\", []))\n                    \n                    interview['first_order_codes'] = codes if isinstance(codes, list) else []\n                    coded_interviews.append(interview)\n                    all_codes.extend(interview['first_order_codes'])\n            else:\n                # Fallback: ajouter sans codes\n                for interview in batch:\n                    interview['first_order_codes'] = []\n                    coded_interviews.append(interview)\n        \n        return coded_interviews, list(set(all_codes))\n    \n    def second_order_coding(self, first_order_codes: List[str]) -> Dict[str, List[str]]:\n        \"\"\"Codage de second ordre - th√®mes √©mergents.\"\"\"\n        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n\nRESEARCH QUESTION:\n{self.research_question}\n\nFIRST-ORDER CODES from {len(first_order_codes)} codes extracted from professional interviews about AI:\n{json.dumps(first_order_codes[:200], ensure_ascii=False, indent=2)}\n\nGroup these first-order codes into SECOND-ORDER THEMES (5-12 themes).\nSecond-order themes are more abstract, researcher-driven concepts that capture:\n- Identity maintenance strategies\n- Threat perception patterns  \n- Legitimacy construction mechanisms\n- Adaptation and resistance behaviors\n- Professional boundary work\n\nRespond with a JSON object:\n{{\n  \"Theme Name\": [\"code 1\", \"code 2\", \"code 3\"],\n  \"Another Theme\": [\"code 4\", \"code 5\"]\n}}\n\"\"\"\n        result = self.gemini.generate_json(prompt)\n        return result if isinstance(result, dict) else {}\n    \n    def aggregate_dimensions(self, second_order_codes: Dict[str, List[str]]) -> Dict[str, List[str]]:\n        \"\"\"Dimensions agr√©g√©es - concepts th√©oriques.\"\"\"\n        prompt = f\"\"\"You are an expert qualitative researcher using the Gioia methodology.\n\nRESEARCH QUESTION:\n{self.research_question}\n\nSECOND-ORDER THEMES:\n{json.dumps(second_order_codes, ensure_ascii=False, indent=2)}\n\nAggregate these themes into 3-5 AGGREGATE DIMENSIONS.\nAggregate dimensions are high-level theoretical constructs that:\n- Answer the research question\n- Connect to existing theory (identity theory, legitimacy theory, technology acceptance)\n- Provide a framework for understanding professional identity negotiation with AI\n\nRespond with a JSON object:\n{{\n  \"Aggregate Dimension Name\": [\"Theme A\", \"Theme B\"],\n  \"Another Dimension\": [\"Theme C\", \"Theme D\"]\n}}\n\"\"\"\n        result = self.gemini.generate_json(prompt)\n        return result if isinstance(result, dict) else {}\n    \n    def run_full_analysis(self, interviews: List[Dict], batch_size: int = 5) -> Dict:\n        \"\"\"Ex√©cute l'analyse Gioia compl√®te avec batch processing.\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(\"üî¨ GIOIA ANALYSIS - Professional Identity & AI\")\n        print(f\"{'='*60}\")\n        print(f\"\\nüìã Research Question:\\n{self.research_question}\\n\")\n        print(f\"üìä {len(interviews)} interviews to analyze (batch size: {batch_size})\")\n        \n        # √âtape 1: Codage initial par batch\n        print(\"\\nüìù STEP 1: First-Order Coding (batch processing)...\")\n        num_batches = (len(interviews) + batch_size - 1) // batch_size\n        print(f\"   Processing {num_batches} batches...\")\n        \n        coded_interviews, unique_first_order = self.batch_initial_coding(interviews, batch_size)\n        print(f\"   ‚úÖ {len(unique_first_order)} unique first-order codes identified\")\n        \n        # √âtape 2: Codage de second ordre\n        print(\"\\nüìù STEP 2: Second-Order Coding...\")\n        second_order = self.second_order_coding(unique_first_order)\n        print(f\"   ‚úÖ {len(second_order)} second-order themes created\")\n        \n        # √âtape 3: Dimensions agr√©g√©es\n        print(\"\\nüìù STEP 3: Aggregate Dimensions...\")\n        dimensions = self.aggregate_dimensions(second_order)\n        print(f\"   ‚úÖ {len(dimensions)} aggregate dimensions created\")\n        \n        return {\n            'interviews': coded_interviews,\n            'first_order_codes': unique_first_order,\n            'second_order_themes': second_order,\n            'aggregate_dimensions': dimensions\n        }\n\n# Instance\ngioia = GioiaAnalysis(gemini, RESEARCH_QUESTION)\nprint(\"‚úÖ GioiaAnalysis initialis√© (avec batch processing)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lancer l'Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ex√©cuter l'analyse Gioia compl√®te (avec batch processing)\nresults = gioia.run_full_analysis(interviews, batch_size=BATCH_SIZE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def display_gioia_structure(results: Dict):\n",
    "    \"\"\"Affiche la structure Gioia compl√®te.\"\"\"\n",
    "    dimensions = results.get('aggregate_dimensions', {})\n",
    "    second_order = results.get('second_order_themes', {})\n",
    "    \n",
    "    html = \"<style>\"\n",
    "    html += \".gioia-table { border-collapse: collapse; width: 100%; font-family: Arial, sans-serif; }\"\n",
    "    html += \".gioia-table th, .gioia-table td { border: 1px solid #ddd; padding: 10px; text-align: left; vertical-align: top; }\"\n",
    "    html += \".gioia-table th { background-color: #4a90d9; color: white; }\"\n",
    "    html += \".dim-cell { background-color: #e8f4e8; font-weight: bold; font-size: 14px; }\"\n",
    "    html += \".theme-cell { background-color: #fff8e8; }\"\n",
    "    html += \".code-cell { font-size: 12px; color: #555; }\"\n",
    "    html += \"</style>\"\n",
    "    \n",
    "    html += \"<h2>üìä Gioia Data Structure</h2>\"\n",
    "    html += \"<table class='gioia-table'>\"\n",
    "    html += \"<tr><th>1st Order Codes</th><th>2nd Order Themes</th><th>Aggregate Dimensions</th></tr>\"\n",
    "    \n",
    "    for dim_name, themes in dimensions.items():\n",
    "        first_row_dim = True\n",
    "        theme_count = len(themes)\n",
    "        \n",
    "        for theme in themes:\n",
    "            first_codes = second_order.get(theme, [])\n",
    "            codes_html = \"<br>\".join([f\"‚Ä¢ {c}\" for c in first_codes[:6]])\n",
    "            if len(first_codes) > 6:\n",
    "                codes_html += f\"<br><i>... +{len(first_codes)-6} more</i>\"\n",
    "            \n",
    "            html += \"<tr>\"\n",
    "            html += f\"<td class='code-cell'>{codes_html}</td>\"\n",
    "            html += f\"<td class='theme-cell'>{theme}</td>\"\n",
    "            \n",
    "            if first_row_dim:\n",
    "                html += f\"<td class='dim-cell' rowspan='{theme_count}'>{dim_name}</td>\"\n",
    "                first_row_dim = False\n",
    "            \n",
    "            html += \"</tr>\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Afficher la structure\n",
    "display_gioia_structure(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les statistiques\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Interviews analyzed: {len(results['interviews'])}\")\n",
    "print(f\"üìù First-order codes: {len(results['first_order_codes'])}\")\n",
    "print(f\"üè∑Ô∏è Second-order themes: {len(results['second_order_themes'])}\")\n",
    "print(f\"üì¶ Aggregate dimensions: {len(results['aggregate_dimensions'])}\")\n",
    "\n",
    "print(\"\\n\\nüì¶ AGGREGATE DIMENSIONS:\")\n",
    "print(\"-\"*40)\n",
    "for dim, themes in results['aggregate_dimensions'].items():\n",
    "    print(f\"\\nüîπ {dim}\")\n",
    "    for theme in themes:\n",
    "        code_count = len(results['second_order_themes'].get(theme, []))\n",
    "        print(f\"   ‚îî‚îÄ {theme} ({code_count} codes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Construction du Mod√®le Th√©orique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_theoretical_model(results: Dict, research_question: str) -> Dict:\n",
    "    \"\"\"Construit un mod√®le th√©orique √† partir des r√©sultats Gioia.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a qualitative research expert building grounded theory.\n",
    "\n",
    "RESEARCH QUESTION:\n",
    "{research_question}\n",
    "\n",
    "GIOIA ANALYSIS RESULTS:\n",
    "\n",
    "Aggregate Dimensions:\n",
    "{json.dumps(results['aggregate_dimensions'], ensure_ascii=False, indent=2)}\n",
    "\n",
    "Second-Order Themes:\n",
    "{json.dumps(results['second_order_themes'], ensure_ascii=False, indent=2)}\n",
    "\n",
    "Based on this analysis, construct a THEORETICAL MODEL that:\n",
    "\n",
    "1. Answers the research question\n",
    "2. Identifies key constructs and their relationships\n",
    "3. Proposes testable propositions\n",
    "4. Connects to existing literature on:\n",
    "   - Professional identity theory\n",
    "   - Legitimacy theory\n",
    "   - Technology acceptance/resistance\n",
    "   - Boundary work in professions\n",
    "\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "  \"model_name\": \"Name of your theoretical model\",\n",
    "  \"core_argument\": \"The central theoretical argument (2-3 sentences)\",\n",
    "  \"constructs\": [\n",
    "    {{\n",
    "      \"name\": \"Construct name\",\n",
    "      \"definition\": \"Definition\",\n",
    "      \"type\": \"independent/dependent/mediator/moderator/process\",\n",
    "      \"grounded_in\": [\"Aggregate dimension(s) it comes from\"]\n",
    "    }}\n",
    "  ],\n",
    "  \"propositions\": [\n",
    "    \"P1: First proposition...\",\n",
    "    \"P2: Second proposition...\"\n",
    "  ],\n",
    "  \"theoretical_contributions\": [\n",
    "    \"Contribution 1\",\n",
    "    \"Contribution 2\"\n",
    "  ],\n",
    "  \"practical_implications\": [\n",
    "    \"Implication 1\",\n",
    "    \"Implication 2\"\n",
    "  ],\n",
    "  \"boundary_conditions\": [\n",
    "    \"When/where this model applies\"\n",
    "  ],\n",
    "  \"future_research\": [\n",
    "    \"Direction 1\",\n",
    "    \"Direction 2\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    model = gemini.generate_json(prompt)\n",
    "    return model if isinstance(model, dict) else {}\n",
    "\n",
    "# Construire le mod√®le\n",
    "print(\"üèóÔ∏è Building theoretical model...\")\n",
    "theoretical_model = build_theoretical_model(results, RESEARCH_QUESTION)\n",
    "print(\"‚úÖ Model constructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le mod√®le th√©orique\n",
    "def display_theoretical_model(model: Dict):\n",
    "    md = f\"\"\"# üß† {model.get('model_name', 'Theoretical Model')}\n",
    "\n",
    "## Core Argument\n",
    "{model.get('core_argument', 'N/A')}\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Constructs\n",
    "\"\"\"\n",
    "    for c in model.get('constructs', []):\n",
    "        md += f\"\\n### {c.get('name')} *({c.get('type')})*\\n\"\n",
    "        md += f\"{c.get('definition')}\\n\"\n",
    "        md += f\"- *Grounded in: {', '.join(c.get('grounded_in', []))}*\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Propositions\\n\"\n",
    "    for p in model.get('propositions', []):\n",
    "        md += f\"- **{p}**\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Theoretical Contributions\\n\"\n",
    "    for c in model.get('theoretical_contributions', []):\n",
    "        md += f\"- {c}\\n\"\n",
    "    \n",
    "    md += \"\\n## Practical Implications\\n\"\n",
    "    for i in model.get('practical_implications', []):\n",
    "        md += f\"- {i}\\n\"\n",
    "    \n",
    "    md += \"\\n## Boundary Conditions\\n\"\n",
    "    for b in model.get('boundary_conditions', []):\n",
    "        md += f\"- {b}\\n\"\n",
    "    \n",
    "    md += \"\\n## Future Research Directions\\n\"\n",
    "    for f in model.get('future_research', []):\n",
    "        md += f\"- {f}\\n\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "\n",
    "display_theoretical_model(theoretical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisation du Mod√®le (Mermaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_diagram(model: Dict) -> str:\n",
    "    \"\"\"G√©n√®re un diagramme Mermaid du mod√®le.\"\"\"\n",
    "    prompt = f\"\"\"Create a Mermaid flowchart diagram for this theoretical model:\n",
    "\n",
    "{json.dumps(model, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Requirements:\n",
    "- Use graph LR (left to right) or graph TD (top down)\n",
    "- Show all constructs as nodes\n",
    "- Show relationships based on propositions as labeled arrows\n",
    "- Use different node shapes for different construct types:\n",
    "  - Independent variables: rectangles\n",
    "  - Dependent variables: rounded rectangles\n",
    "  - Mediators: circles\n",
    "  - Moderators: diamonds\n",
    "- Add colors using style definitions\n",
    "\n",
    "Return ONLY the Mermaid code, no markdown markers.\n",
    "\"\"\"\n",
    "    \n",
    "    response = gemini.generate(prompt, temperature=0.3)\n",
    "    \n",
    "    # Nettoyer\n",
    "    cleaned = response.strip()\n",
    "    if cleaned.startswith('```mermaid'):\n",
    "        cleaned = cleaned[10:]\n",
    "    if cleaned.startswith('```'):\n",
    "        cleaned = cleaned[3:]\n",
    "    if cleaned.endswith('```'):\n",
    "        cleaned = cleaned[:-3]\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "# G√©n√©rer le diagramme\n",
    "print(\"üìä Generating model diagram...\")\n",
    "mermaid_code = generate_model_diagram(theoretical_model)\n",
    "print(\"\\nüìà Mermaid Diagram Code:\")\n",
    "print(\"-\"*40)\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le diagramme (fonctionne dans Colab/Jupyter)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "html = f\"\"\"\n",
    "<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
    "<script>mermaid.initialize({{startOnLoad:true, theme:'default'}});</script>\n",
    "<div class=\"mermaid\">\n",
    "{mermaid_code}\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Compiler tous les r√©sultats\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'research_question': RESEARCH_QUESTION,\n",
    "        'dataset': 'Anthropic/AnthropicInterviewer',\n",
    "        'split': 'workforce',\n",
    "        'sample_size': len(interviews),\n",
    "        'model': MODEL_NAME,\n",
    "        'analysis_date': datetime.now().isoformat()\n",
    "    },\n",
    "    'gioia_analysis': {\n",
    "        'first_order_codes': results['first_order_codes'],\n",
    "        'second_order_themes': results['second_order_themes'],\n",
    "        'aggregate_dimensions': results['aggregate_dimensions']\n",
    "    },\n",
    "    'theoretical_model': theoretical_model,\n",
    "    'visualization': {\n",
    "        'mermaid_diagram': mermaid_code\n",
    "    },\n",
    "    'coded_interviews': [\n",
    "        {\n",
    "            'id': i['id'],\n",
    "            'first_order_codes': i.get('first_order_codes', [])\n",
    "        } for i in results['interviews']\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "filename = f\"grounded_theory_professional_identity_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {filename}\")\n",
    "\n",
    "# T√©l√©charger (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    print(\"üì• Download started...\")\n",
    "except:\n",
    "    print(\"(Manual download required outside Colab)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV des codes pour analyse suppl√©mentaire\n",
    "codes_data = []\n",
    "for theme, codes in results['second_order_themes'].items():\n",
    "    # Trouver la dimension agr√©g√©e\n",
    "    dimension = None\n",
    "    for dim, themes in results['aggregate_dimensions'].items():\n",
    "        if theme in themes:\n",
    "            dimension = dim\n",
    "            break\n",
    "    \n",
    "    for code in codes:\n",
    "        codes_data.append({\n",
    "            'first_order_code': code,\n",
    "            'second_order_theme': theme,\n",
    "            'aggregate_dimension': dimension\n",
    "        })\n",
    "\n",
    "df_codes = pd.DataFrame(codes_data)\n",
    "csv_filename = f\"gioia_codes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_codes.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n‚úÖ Codes exported to: {csv_filename}\")\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "display(df_codes.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ Notes\n",
    "\n",
    "### M√©thode Gioia\n",
    "La m√©thode Gioia est une approche rigoureuse pour d√©velopper une th√©orie ancr√©e:\n",
    "1. **Codes de 1er ordre**: Proches des mots des participants\n",
    "2. **Th√®mes de 2nd ordre**: Concepts plus abstraits du chercheur\n",
    "3. **Dimensions agr√©g√©es**: Construits th√©oriques de haut niveau\n",
    "\n",
    "### Personnalisation\n",
    "- Modifiez `SAMPLE_SIZE` pour analyser plus/moins d'interviews\n",
    "- Adaptez `RESEARCH_QUESTION` pour changer le focus\n",
    "- Le mod√®le peut √™tre chang√© dans la configuration\n",
    "\n",
    "### R√©f√©rences\n",
    "- Gioia, D. A., Corley, K. G., & Hamilton, A. L. (2013). Seeking qualitative rigor in inductive research\n",
    "- Dataset: [Anthropic/AnthropicInterviewer](https://huggingface.co/datasets/Anthropic/AnthropicInterviewer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}