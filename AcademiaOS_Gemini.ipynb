{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì AcademiaOS - Version Gemini\n",
    "\n",
    "**Automatisation du d√©veloppement de la Grounded Theory avec Gemini**\n",
    "\n",
    "Ce notebook reproduit les fonctionnalit√©s d'[AcademiaOS](https://github.com/thomasuebi/academia-os) en utilisant l'API Gemini au lieu d'OpenAI.\n",
    "\n",
    "## Fonctionnalit√©s\n",
    "- üìö Recherche d'articles acad√©miques via Semantic Scholar\n",
    "- üìù Codage qualitatif automatis√© (m√©thode Gioia)\n",
    "- üîç Extraction d'informations structur√©es\n",
    "- üß† Construction th√©orique automatis√©e\n",
    "- üìä Visualisation des relations conceptuelles\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai\n",
    "!pip install -q semanticscholar\n",
    "!pip install -q PyPDF2\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q numpy\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'API Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configuration de la cl√© API\n",
    "# Option 1: Via les secrets Colab (recommand√©)\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    # Option 2: Saisie manuelle\n",
    "    GOOGLE_API_KEY = input(\"Entrez votre cl√© API Gemini: \")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Configuration du mod√®le\n",
    "MODEL_NAME = \"gemini-1.5-pro\"  # ou \"gemini-1.5-flash\" pour plus de rapidit√©\n",
    "EMBEDDING_MODEL = \"models/embedding-001\"\n",
    "\n",
    "print(f\"‚úÖ API Gemini configur√©e avec le mod√®le: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Service Gemini (√©quivalent √† OpenAIService)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiService:\n",
    "    \"\"\"Service principal pour les appels √† l'API Gemini.\n",
    "    \n",
    "    √âquivalent du OpenAIService d'AcademiaOS, adapt√© pour Gemini.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = MODEL_NAME):\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            top_k=40,\n",
    "            max_output_tokens=8192,\n",
    "        )\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.7) -> str:\n",
    "        \"\"\"G√©n√®re une r√©ponse √† partir d'un prompt.\"\"\"\n",
    "        config = genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            max_output_tokens=8192,\n",
    "        )\n",
    "        response = self.model.generate_content(prompt, generation_config=config)\n",
    "        return response.text\n",
    "    \n",
    "    def generate_json(self, prompt: str, temperature: float = 0.3) -> Any:\n",
    "        \"\"\"G√©n√®re une r√©ponse JSON structur√©e.\"\"\"\n",
    "        json_prompt = f\"\"\"{prompt}\n",
    "\n",
    "IMPORTANT: R√©ponds UNIQUEMENT avec un JSON valide, sans texte avant ou apr√®s.\n",
    "Ne commence pas par ```json et ne termine pas par ```.\n",
    "\"\"\"\n",
    "        response = self.generate(json_prompt, temperature=temperature)\n",
    "        \n",
    "        # Nettoyer la r√©ponse\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith('```json'):\n",
    "            cleaned = cleaned[7:]\n",
    "        if cleaned.startswith('```'):\n",
    "            cleaned = cleaned[3:]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned.strip())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur de parsing JSON: {e}\")\n",
    "            print(f\"R√©ponse brute: {response[:500]}...\")\n",
    "            return None\n",
    "    \n",
    "    def stream_generate(self, prompt: str, callback: Callable[[str], None] = None):\n",
    "        \"\"\"G√©n√®re une r√©ponse en streaming.\"\"\"\n",
    "        response = self.model.generate_content(prompt, stream=True)\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.text:\n",
    "                full_response += chunk.text\n",
    "                if callback:\n",
    "                    callback(chunk.text)\n",
    "        return full_response\n",
    "\n",
    "# Instance globale\n",
    "gemini_service = GeminiService()\n",
    "print(\"‚úÖ GeminiService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Service d'Embeddings et Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingService:\n",
    "    \"\"\"Service pour les embeddings et le ranking s√©mantique.\n",
    "    \n",
    "    Remplace OpenAIEmbeddings de LangChain par les embeddings Gemini.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = EMBEDDING_MODEL):\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        \"\"\"G√©n√®re un embedding pour un texte.\"\"\"\n",
    "        result = genai.embed_content(\n",
    "            model=self.model,\n",
    "            content=text,\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "        return result['embedding']\n",
    "    \n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        \"\"\"G√©n√®re un embedding pour une requ√™te.\"\"\"\n",
    "        result = genai.embed_content(\n",
    "            model=self.model,\n",
    "            content=query,\n",
    "            task_type=\"retrieval_query\"\n",
    "        )\n",
    "        return result['embedding']\n",
    "    \n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"G√©n√®re des embeddings pour plusieurs textes.\"\"\"\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts, desc=\"G√©n√©ration des embeddings\"):\n",
    "            embeddings.append(self.embed_text(text))\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class RankingService:\n",
    "    \"\"\"Service de ranking s√©mantique des articles.\n",
    "    \n",
    "    √âquivalent du RankingService d'AcademiaOS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_service = EmbeddingService()\n",
    "    \n",
    "    def rank_papers(self, query: str, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Classe les articles par pertinence s√©mantique.\"\"\"\n",
    "        if not query or not papers:\n",
    "            return papers\n",
    "        \n",
    "        # Cr√©er les textes √† embedder\n",
    "        texts = []\n",
    "        for paper in papers:\n",
    "            text = f\"{paper.get('title', '')} {paper.get('abstract', '')}\"\n",
    "            texts.append(text[:2000])  # Limiter la taille\n",
    "        \n",
    "        # G√©n√©rer les embeddings\n",
    "        query_embedding = self.embedding_service.embed_query(query)\n",
    "        paper_embeddings = self.embedding_service.embed_texts(texts)\n",
    "        \n",
    "        # Calculer les similarit√©s\n",
    "        similarities = cosine_similarity(\n",
    "            [query_embedding], \n",
    "            paper_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Trier par similarit√©\n",
    "        ranked_indices = np.argsort(similarities)[::-1]\n",
    "        ranked_papers = [papers[i] for i in ranked_indices]\n",
    "        \n",
    "        # Ajouter les scores\n",
    "        for i, idx in enumerate(ranked_indices):\n",
    "            ranked_papers[i]['similarity_score'] = float(similarities[idx])\n",
    "        \n",
    "        return ranked_papers\n",
    "\n",
    "# Instances globales\n",
    "embedding_service = EmbeddingService()\n",
    "ranking_service = RankingService()\n",
    "print(\"‚úÖ Services d'Embedding et Ranking initialis√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Service de Recherche Acad√©mique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "\n",
    "class SearchService:\n",
    "    \"\"\"Service de recherche d'articles acad√©miques via Semantic Scholar.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sch = SemanticScholar()\n",
    "    \n",
    "    def search_papers(self, query: str, limit: int = 20) -> List[Dict]:\n",
    "        \"\"\"Recherche des articles acad√©miques.\"\"\"\n",
    "        try:\n",
    "            results = self.sch.search_paper(query, limit=limit)\n",
    "            papers = []\n",
    "            for paper in results:\n",
    "                papers.append({\n",
    "                    'id': paper.paperId,\n",
    "                    'title': paper.title,\n",
    "                    'abstract': paper.abstract or '',\n",
    "                    'year': paper.year,\n",
    "                    'citation_count': paper.citationCount,\n",
    "                    'authors': [a.name for a in (paper.authors or [])],\n",
    "                    'url': paper.url,\n",
    "                    'venue': getattr(paper, 'venue', ''),\n",
    "                })\n",
    "            return papers\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur de recherche: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_paper_details(self, paper_id: str) -> Optional[Dict]:\n",
    "        \"\"\"R√©cup√®re les d√©tails d'un article.\"\"\"\n",
    "        try:\n",
    "            paper = self.sch.get_paper(paper_id)\n",
    "            return {\n",
    "                'id': paper.paperId,\n",
    "                'title': paper.title,\n",
    "                'abstract': paper.abstract or '',\n",
    "                'year': paper.year,\n",
    "                'citation_count': paper.citationCount,\n",
    "                'authors': [a.name for a in (paper.authors or [])],\n",
    "                'url': paper.url,\n",
    "                'tldr': getattr(paper, 'tldr', {}).get('text', '') if hasattr(paper, 'tldr') and paper.tldr else '',\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "            return None\n",
    "\n",
    "search_service = SearchService()\n",
    "print(\"‚úÖ SearchService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Codage Qualitatif - M√©thode Gioia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GioiaCodingService:\n",
    "    \"\"\"Service de codage qualitatif selon la m√©thode Gioia.\n",
    "    \n",
    "    La m√©thode Gioia comprend 3 niveaux de codage:\n",
    "    1. Codes de premier ordre (concepts √©mergents des donn√©es)\n",
    "    2. Codes de second ordre (th√®mes agr√©g√©s)\n",
    "    3. Dimensions agr√©g√©es (concepts th√©oriques)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_service: GeminiService):\n",
    "        self.gemini = gemini_service\n",
    "    \n",
    "    def initial_coding(self, paper: Dict, remarks: str = \"\") -> List[str]:\n",
    "        \"\"\"Codage de premier ordre d'un article.\n",
    "        \n",
    "        Identifie les concepts et th√®mes √©mergents du texte.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Tu es un chercheur qualitatif expert utilisant la m√©thode Gioia.\n",
    "\n",
    "Analyse le texte suivant et identifie les CODES DE PREMIER ORDRE.\n",
    "Les codes de premier ordre sont des concepts et th√®mes √©mergents directement issus des donn√©es,\n",
    "exprim√©s dans le langage proche des donn√©es sources.\n",
    "\n",
    "TITRE: {paper.get('title', 'N/A')}\n",
    "\n",
    "ABSTRACT: {paper.get('abstract', 'N/A')}\n",
    "\n",
    "TEXTE COMPLET: {paper.get('fullText', paper.get('abstract', ''))[:8000]}\n",
    "\n",
    "{f'REMARQUES DU CHERCHEUR: {remarks}' if remarks else ''}\n",
    "\n",
    "Identifie entre 5 et 15 codes de premier ordre pertinents.\n",
    "R√©ponds UNIQUEMENT avec un tableau JSON de strings, par exemple:\n",
    "[\"code 1\", \"code 2\", \"code 3\"]\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, list) else []\n",
    "    \n",
    "    def second_order_coding(self, first_order_codes: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Codage de second ordre.\n",
    "        \n",
    "        Agr√®ge les codes de premier ordre en th√®mes plus abstraits.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Tu es un chercheur qualitatif expert utilisant la m√©thode Gioia.\n",
    "\n",
    "Voici des CODES DE PREMIER ORDRE issus d'une analyse qualitative:\n",
    "{json.dumps(first_order_codes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Regroupe ces codes en CODES DE SECOND ORDRE (th√®mes plus abstraits).\n",
    "Les codes de second ordre repr√©sentent des patterns et th√®mes qui √©mergent\n",
    "de l'agr√©gation des codes de premier ordre.\n",
    "\n",
    "R√©ponds avec un objet JSON o√π:\n",
    "- Les cl√©s sont les codes de second ordre (th√®mes)\n",
    "- Les valeurs sont des tableaux de codes de premier ordre correspondants\n",
    "\n",
    "Exemple de format:\n",
    "{{\n",
    "  \"Th√®me A\": [\"code 1\", \"code 2\"],\n",
    "  \"Th√®me B\": [\"code 3\", \"code 4\", \"code 5\"]\n",
    "}}\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, dict) else {}\n",
    "    \n",
    "    def aggregate_dimensions(self, second_order_codes: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Agr√©gation en dimensions th√©oriques.\n",
    "        \n",
    "        Cr√©e des dimensions th√©oriques √† partir des codes de second ordre.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Tu es un chercheur qualitatif expert utilisant la m√©thode Gioia.\n",
    "\n",
    "Voici des CODES DE SECOND ORDRE issus d'une analyse qualitative:\n",
    "{json.dumps(second_order_codes, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Agr√®ge ces codes en DIMENSIONS AGR√âG√âES (concepts th√©oriques de haut niveau).\n",
    "Les dimensions agr√©g√©es repr√©sentent les construits th√©oriques fondamentaux\n",
    "qui √©mergent de l'analyse.\n",
    "\n",
    "R√©ponds avec un objet JSON o√π:\n",
    "- Les cl√©s sont les dimensions agr√©g√©es\n",
    "- Les valeurs sont des tableaux de codes de second ordre correspondants\n",
    "\n",
    "Exemple de format:\n",
    "{{\n",
    "  \"Dimension Th√©orique 1\": [\"Th√®me A\", \"Th√®me B\"],\n",
    "  \"Dimension Th√©orique 2\": [\"Th√®me C\", \"Th√®me D\"]\n",
    "}}\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, dict) else {}\n",
    "    \n",
    "    def full_coding_process(self, papers: List[Dict], remarks: str = \"\") -> Dict:\n",
    "        \"\"\"Ex√©cute le processus complet de codage Gioia.\"\"\"\n",
    "        print(\"üìù √âtape 1: Codage de premier ordre...\")\n",
    "        all_first_order = []\n",
    "        for paper in tqdm(papers, desc=\"Codage initial\"):\n",
    "            codes = self.initial_coding(paper, remarks)\n",
    "            paper['initial_codes'] = codes\n",
    "            all_first_order.extend(codes)\n",
    "        \n",
    "        # D√©dupliquer\n",
    "        unique_first_order = list(set(all_first_order))\n",
    "        print(f\"   ‚úÖ {len(unique_first_order)} codes de premier ordre identifi√©s\")\n",
    "        \n",
    "        print(\"\\nüìù √âtape 2: Codage de second ordre...\")\n",
    "        second_order = self.second_order_coding(unique_first_order)\n",
    "        print(f\"   ‚úÖ {len(second_order)} th√®mes de second ordre cr√©√©s\")\n",
    "        \n",
    "        print(\"\\nüìù √âtape 3: Dimensions agr√©g√©es...\")\n",
    "        dimensions = self.aggregate_dimensions(second_order)\n",
    "        print(f\"   ‚úÖ {len(dimensions)} dimensions agr√©g√©es cr√©√©es\")\n",
    "        \n",
    "        return {\n",
    "            'papers': papers,\n",
    "            'first_order_codes': unique_first_order,\n",
    "            'second_order_codes': second_order,\n",
    "            'aggregate_dimensions': dimensions\n",
    "        }\n",
    "\n",
    "gioia_service = GioiaCodingService(gemini_service)\n",
    "print(\"‚úÖ GioiaCodingService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Construction de Mod√®le Th√©orique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingService:\n",
    "    \"\"\"Service de construction de mod√®le th√©orique.\n",
    "    \n",
    "    √âquivalent des fonctions de modeling d'AcademiaOS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_service: GeminiService):\n",
    "        self.gemini = gemini_service\n",
    "    \n",
    "    def brainstorm_theories(self, coding_results: Dict, remarks: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Identifie les th√©ories applicables aux donn√©es.\"\"\"\n",
    "        prompt = f\"\"\"Tu es un chercheur expert en th√©orie organisationnelle et sciences sociales.\n",
    "\n",
    "Analyse ces r√©sultats de codage qualitatif:\n",
    "\n",
    "DIMENSIONS AGR√âG√âES:\n",
    "{json.dumps(coding_results.get('aggregate_dimensions', {}), ensure_ascii=False, indent=2)}\n",
    "\n",
    "CODES DE SECOND ORDRE:\n",
    "{json.dumps(coding_results.get('second_order_codes', {}), ensure_ascii=False, indent=2)}\n",
    "\n",
    "{f'CONTEXTE DE RECHERCHE: {remarks}' if remarks else ''}\n",
    "\n",
    "Identifie 3 √† 5 th√©ories existantes qui pourraient s'appliquer √† ces donn√©es.\n",
    "Pour chaque th√©orie, explique pourquoi elle est pertinente.\n",
    "\n",
    "R√©ponds avec un tableau JSON:\n",
    "[\n",
    "  {{\n",
    "    \"theory_name\": \"Nom de la th√©orie\",\n",
    "    \"description\": \"Br√®ve description\",\n",
    "    \"relevance\": \"Pourquoi cette th√©orie s'applique\",\n",
    "    \"key_concepts\": [\"concept1\", \"concept2\"]\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, list) else []\n",
    "    \n",
    "    def identify_relationships(self, coding_results: Dict) -> List[Dict]:\n",
    "        \"\"\"Identifie les relations entre concepts.\"\"\"\n",
    "        prompt = f\"\"\"Analyse ces dimensions et th√®mes issus d'une recherche qualitative:\n",
    "\n",
    "DIMENSIONS AGR√âG√âES:\n",
    "{json.dumps(coding_results.get('aggregate_dimensions', {}), ensure_ascii=False, indent=2)}\n",
    "\n",
    "CODES DE SECOND ORDRE:\n",
    "{json.dumps(coding_results.get('second_order_codes', {}), ensure_ascii=False, indent=2)}\n",
    "\n",
    "Identifie les RELATIONS potentielles entre ces concepts.\n",
    "Pour chaque relation, sp√©cifie:\n",
    "- Les concepts li√©s\n",
    "- Le type de relation (causal, corr√©lation, mod√©ration, m√©diation)\n",
    "- Une hypoth√®se de recherche\n",
    "\n",
    "R√©ponds avec un tableau JSON:\n",
    "[\n",
    "  {{\n",
    "    \"concept_a\": \"Premier concept\",\n",
    "    \"concept_b\": \"Second concept\",\n",
    "    \"relationship_type\": \"type de relation\",\n",
    "    \"hypothesis\": \"Hypoth√®se formul√©e\",\n",
    "    \"direction\": \"A -> B\" ou \"A <-> B\"\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, list) else []\n",
    "    \n",
    "    def construct_model(self, coding_results: Dict, theories: List[Dict], \n",
    "                       relationships: List[Dict], remarks: str = \"\") -> Dict:\n",
    "        \"\"\"Construit un mod√®le th√©orique int√©gr√©.\"\"\"\n",
    "        prompt = f\"\"\"Tu es un chercheur expert en construction th√©orique.\n",
    "\n",
    "√Ä partir des √©l√©ments suivants, construis un MOD√àLE TH√âORIQUE int√©gr√©:\n",
    "\n",
    "DIMENSIONS AGR√âG√âES:\n",
    "{json.dumps(coding_results.get('aggregate_dimensions', {}), ensure_ascii=False, indent=2)}\n",
    "\n",
    "TH√âORIES APPLICABLES:\n",
    "{json.dumps(theories, ensure_ascii=False, indent=2)}\n",
    "\n",
    "RELATIONS IDENTIFI√âES:\n",
    "{json.dumps(relationships, ensure_ascii=False, indent=2)}\n",
    "\n",
    "{f'CONTEXTE: {remarks}' if remarks else ''}\n",
    "\n",
    "Construis un mod√®le th√©orique coh√©rent qui:\n",
    "1. Int√®gre les dimensions cl√©s\n",
    "2. S'appuie sur les th√©ories pertinentes\n",
    "3. Propose des relations causales testables\n",
    "\n",
    "R√©ponds avec un objet JSON:\n",
    "{{\n",
    "  \"model_name\": \"Nom du mod√®le\",\n",
    "  \"core_proposition\": \"Proposition centrale du mod√®le\",\n",
    "  \"constructs\": [\n",
    "    {{\n",
    "      \"name\": \"Nom du construit\",\n",
    "      \"definition\": \"D√©finition\",\n",
    "      \"type\": \"independent/dependent/mediator/moderator\"\n",
    "    }}\n",
    "  ],\n",
    "  \"propositions\": [\n",
    "    \"P1: Proposition 1\",\n",
    "    \"P2: Proposition 2\"\n",
    "  ],\n",
    "  \"theoretical_contribution\": \"Contribution th√©orique\",\n",
    "  \"boundary_conditions\": [\"Condition 1\", \"Condition 2\"]\n",
    "}}\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, dict) else {}\n",
    "    \n",
    "    def critique_model(self, model: Dict) -> Dict:\n",
    "        \"\"\"Critique le mod√®le th√©orique.\"\"\"\n",
    "        prompt = f\"\"\"Tu es un reviewer acad√©mique exp√©riment√©.\n",
    "\n",
    "Critique ce mod√®le th√©orique de mani√®re constructive:\n",
    "{json.dumps(model, ensure_ascii=False, indent=2)}\n",
    "\n",
    "√âvalue:\n",
    "1. La coh√©rence logique\n",
    "2. L'ancrage th√©orique\n",
    "3. La testabilit√© des propositions\n",
    "4. Les limites potentielles\n",
    "5. Les am√©liorations sugg√©r√©es\n",
    "\n",
    "R√©ponds avec un objet JSON:\n",
    "{{\n",
    "  \"overall_assessment\": \"√âvaluation g√©n√©rale\",\n",
    "  \"strengths\": [\"Force 1\", \"Force 2\"],\n",
    "  \"weaknesses\": [\"Faiblesse 1\", \"Faiblesse 2\"],\n",
    "  \"suggestions\": [\"Suggestion 1\", \"Suggestion 2\"],\n",
    "  \"score\": 7  // Note sur 10\n",
    "}}\n",
    "\"\"\"\n",
    "        result = self.gemini.generate_json(prompt)\n",
    "        return result if isinstance(result, dict) else {}\n",
    "    \n",
    "    def generate_mermaid_diagram(self, model: Dict) -> str:\n",
    "        \"\"\"G√©n√®re un diagramme Mermaid du mod√®le.\"\"\"\n",
    "        prompt = f\"\"\"Cr√©e un diagramme Mermaid pour visualiser ce mod√®le th√©orique:\n",
    "{json.dumps(model, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Utilise la syntaxe Mermaid flowchart (graph TD ou graph LR).\n",
    "Inclus:\n",
    "- Les construits comme n≈ìuds\n",
    "- Les relations comme fl√®ches avec labels\n",
    "- Des couleurs pour distinguer les types de construits\n",
    "\n",
    "R√©ponds UNIQUEMENT avec le code Mermaid, sans balises de code.\n",
    "\"\"\"\n",
    "        response = self.gemini.generate(prompt, temperature=0.3)\n",
    "        # Nettoyer\n",
    "        cleaned = response.strip()\n",
    "        if cleaned.startswith('```mermaid'):\n",
    "            cleaned = cleaned[10:]\n",
    "        if cleaned.startswith('```'):\n",
    "            cleaned = cleaned[3:]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        return cleaned.strip()\n",
    "\n",
    "modeling_service = ModelingService(gemini_service)\n",
    "print(\"‚úÖ ModelingService initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def display_gioia_structure(coding_results: Dict):\n",
    "    \"\"\"Affiche la structure de donn√©es Gioia.\"\"\"\n",
    "    dimensions = coding_results.get('aggregate_dimensions', {})\n",
    "    second_order = coding_results.get('second_order_codes', {})\n",
    "    \n",
    "    html = \"<div style='font-family: Arial, sans-serif;'>\"\n",
    "    html += \"<h3>üìä Structure de Codage Gioia</h3>\"\n",
    "    html += \"<table style='border-collapse: collapse; width: 100%;'>\"\n",
    "    html += \"<tr style='background-color: #f0f0f0;'>\"\n",
    "    html += \"<th style='border: 1px solid #ddd; padding: 12px; text-align: left;'>Codes 1er Ordre</th>\"\n",
    "    html += \"<th style='border: 1px solid #ddd; padding: 12px; text-align: left;'>Th√®mes 2nd Ordre</th>\"\n",
    "    html += \"<th style='border: 1px solid #ddd; padding: 12px; text-align: left;'>Dimensions Agr√©g√©es</th>\"\n",
    "    html += \"</tr>\"\n",
    "    \n",
    "    for dim_name, themes in dimensions.items():\n",
    "        first_row = True\n",
    "        for theme in themes:\n",
    "            first_codes = second_order.get(theme, [])\n",
    "            codes_html = \"<br>\".join([f\"‚Ä¢ {c}\" for c in first_codes[:5]])\n",
    "            if len(first_codes) > 5:\n",
    "                codes_html += f\"<br>... (+{len(first_codes)-5} autres)\"\n",
    "            \n",
    "            html += \"<tr>\"\n",
    "            html += f\"<td style='border: 1px solid #ddd; padding: 8px; font-size: 12px;'>{codes_html}</td>\"\n",
    "            html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{theme}</td>\"\n",
    "            if first_row:\n",
    "                html += f\"<td style='border: 1px solid #ddd; padding: 8px; background-color: #e8f4e8; font-weight: bold;' rowspan='{len(themes)}'>{dim_name}</td>\"\n",
    "                first_row = False\n",
    "            html += \"</tr>\"\n",
    "    \n",
    "    html += \"</table></div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def display_mermaid(mermaid_code: str):\n",
    "    \"\"\"Affiche un diagramme Mermaid.\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
    "    <script>mermaid.initialize({{startOnLoad:true}});</script>\n",
    "    <div class=\"mermaid\">\n",
    "    {mermaid_code}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def display_model(model: Dict):\n",
    "    \"\"\"Affiche le mod√®le th√©orique.\"\"\"\n",
    "    md = f\"\"\"## üß† {model.get('model_name', 'Mod√®le Th√©orique')}\n",
    "\n",
    "### Proposition Centrale\n",
    "{model.get('core_proposition', 'N/A')}\n",
    "\n",
    "### Construits\n",
    "\"\"\"\n",
    "    for construct in model.get('constructs', []):\n",
    "        md += f\"- **{construct.get('name')}** ({construct.get('type')}): {construct.get('definition')}\\n\"\n",
    "    \n",
    "    md += \"\\n### Propositions\\n\"\n",
    "    for prop in model.get('propositions', []):\n",
    "        md += f\"- {prop}\\n\"\n",
    "    \n",
    "    md += f\"\\n### Contribution Th√©orique\\n{model.get('theoretical_contribution', 'N/A')}\\n\"\n",
    "    \n",
    "    if model.get('boundary_conditions'):\n",
    "        md += \"\\n### Conditions Limites\\n\"\n",
    "        for bc in model.get('boundary_conditions', []):\n",
    "            md += f\"- {bc}\\n\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "\n",
    "print(\"‚úÖ Fonctions de visualisation charg√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üöÄ Utilisation - Exemple Complet\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 1: Recherche d'articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir votre sujet de recherche\n",
    "RESEARCH_TOPIC = \"digital transformation organizational change\"  # Modifiez selon votre sujet\n",
    "RESEARCHER_REMARKS = \"Je m'int√©resse aux facteurs de succ√®s de la transformation digitale dans les grandes entreprises.\"\n",
    "\n",
    "# Rechercher des articles\n",
    "print(f\"üîç Recherche d'articles sur: {RESEARCH_TOPIC}\")\n",
    "papers = search_service.search_papers(RESEARCH_TOPIC, limit=15)\n",
    "print(f\"‚úÖ {len(papers)} articles trouv√©s\")\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "df = pd.DataFrame(papers)[['title', 'year', 'citation_count', 'authors']]\n",
    "df['authors'] = df['authors'].apply(lambda x: ', '.join(x[:2]) + ('...' if len(x) > 2 else ''))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 2: Ranking s√©mantique (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classer les articles par pertinence\n",
    "print(\"üìä Ranking des articles par pertinence s√©mantique...\")\n",
    "ranked_papers = ranking_service.rank_papers(\n",
    "    query=RESEARCHER_REMARKS,\n",
    "    papers=papers\n",
    ")\n",
    "\n",
    "# Garder les top articles\n",
    "TOP_N = 10\n",
    "selected_papers = ranked_papers[:TOP_N]\n",
    "print(f\"\\n‚úÖ Top {TOP_N} articles s√©lectionn√©s:\")\n",
    "for i, p in enumerate(selected_papers, 1):\n",
    "    print(f\"{i}. [{p.get('similarity_score', 0):.3f}] {p['title'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 3: Codage Gioia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cuter le processus de codage complet\n",
    "print(\"üî¨ Lancement du codage Gioia...\\n\")\n",
    "coding_results = gioia_service.full_coding_process(\n",
    "    papers=selected_papers,\n",
    "    remarks=RESEARCHER_REMARKS\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìã R√âSUM√â DU CODAGE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Codes de 1er ordre: {len(coding_results['first_order_codes'])}\")\n",
    "print(f\"Th√®mes de 2nd ordre: {len(coding_results['second_order_codes'])}\")\n",
    "print(f\"Dimensions agr√©g√©es: {len(coding_results['aggregate_dimensions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la structure Gioia\n",
    "display_gioia_structure(coding_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 4: Construction du mod√®le th√©orique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brainstorming de th√©ories applicables\n",
    "print(\"üß† Identification des th√©ories applicables...\")\n",
    "theories = modeling_service.brainstorm_theories(coding_results, RESEARCHER_REMARKS)\n",
    "\n",
    "print(f\"\\n‚úÖ {len(theories)} th√©ories identifi√©es:\")\n",
    "for t in theories:\n",
    "    print(f\"\\nüìö {t.get('theory_name')}\")\n",
    "    print(f\"   {t.get('relevance', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des relations\n",
    "print(\"üîó Identification des relations entre concepts...\")\n",
    "relationships = modeling_service.identify_relationships(coding_results)\n",
    "\n",
    "print(f\"\\n‚úÖ {len(relationships)} relations identifi√©es:\")\n",
    "for r in relationships[:5]:\n",
    "    print(f\"   {r.get('concept_a')} {r.get('direction', '->')} {r.get('concept_b')}\")\n",
    "    print(f\"   Type: {r.get('relationship_type')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du mod√®le\n",
    "print(\"üèóÔ∏è Construction du mod√®le th√©orique...\")\n",
    "model = modeling_service.construct_model(\n",
    "    coding_results=coding_results,\n",
    "    theories=theories,\n",
    "    relationships=relationships,\n",
    "    remarks=RESEARCHER_REMARKS\n",
    ")\n",
    "\n",
    "# Afficher le mod√®le\n",
    "display_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du mod√®le\n",
    "print(\"üìä G√©n√©ration du diagramme...\")\n",
    "mermaid_code = modeling_service.generate_mermaid_diagram(model)\n",
    "print(\"\\nCode Mermaid g√©n√©r√©:\")\n",
    "print(mermaid_code)\n",
    "\n",
    "# Afficher le diagramme\n",
    "display_mermaid(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √âtape 5: Critique du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critique du mod√®le\n",
    "print(\"üîç Analyse critique du mod√®le...\")\n",
    "critique = modeling_service.critique_model(model)\n",
    "\n",
    "print(f\"\\nüìã CRITIQUE DU MOD√àLE (Score: {critique.get('score', 'N/A')}/10)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìù √âvaluation: {critique.get('overall_assessment', 'N/A')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Points forts:\")\n",
    "for s in critique.get('strengths', []):\n",
    "    print(f\"   ‚Ä¢ {s}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Points faibles:\")\n",
    "for w in critique.get('weaknesses', []):\n",
    "    print(f\"   ‚Ä¢ {w}\")\n",
    "\n",
    "print(\"\\nüí° Suggestions:\")\n",
    "for sug in critique.get('suggestions', []):\n",
    "    print(f\"   ‚Ä¢ {sug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Export des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder tous les r√©sultats\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    'metadata': {\n",
    "        'research_topic': RESEARCH_TOPIC,\n",
    "        'remarks': RESEARCHER_REMARKS,\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'model': MODEL_NAME\n",
    "    },\n",
    "    'papers': [{'id': p['id'], 'title': p['title'], 'initial_codes': p.get('initial_codes', [])} \n",
    "              for p in selected_papers],\n",
    "    'coding_results': {\n",
    "        'first_order_codes': coding_results['first_order_codes'],\n",
    "        'second_order_codes': coding_results['second_order_codes'],\n",
    "        'aggregate_dimensions': coding_results['aggregate_dimensions']\n",
    "    },\n",
    "    'theories': theories,\n",
    "    'relationships': relationships,\n",
    "    'model': model,\n",
    "    'critique': critique,\n",
    "    'mermaid_diagram': mermaid_code\n",
    "}\n",
    "\n",
    "# Sauvegarder en JSON\n",
    "filename = f\"grounded_theory_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s dans: {filename}\")\n",
    "\n",
    "# T√©l√©charger le fichier (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "except:\n",
    "    print(\"(T√©l√©chargement automatique non disponible hors Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ Notes d'utilisation\n",
    "\n",
    "### Cl√© API Gemini\n",
    "1. Allez sur [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
    "2. Cr√©ez une cl√© API\n",
    "3. Dans Colab: `Secrets` (üîë) ‚Üí Ajoutez `GOOGLE_API_KEY`\n",
    "\n",
    "### Mod√®les disponibles\n",
    "- `gemini-1.5-pro` : Plus pr√©cis, recommand√© pour l'analyse qualitative\n",
    "- `gemini-1.5-flash` : Plus rapide, bon pour les tests\n",
    "\n",
    "### Personnalisation\n",
    "- Modifiez `RESEARCH_TOPIC` pour votre sujet\n",
    "- Ajoutez des `RESEARCHER_REMARKS` pour guider l'analyse\n",
    "- Ajustez le nombre d'articles avec `limit` et `TOP_N`\n",
    "\n",
    "### Limitations\n",
    "- Semantic Scholar peut avoir des limites de requ√™tes\n",
    "- L'analyse d√©pend de la qualit√© des abstracts disponibles\n",
    "- Pour de meilleurs r√©sultats, ajoutez le texte complet des articles (`fullText`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
