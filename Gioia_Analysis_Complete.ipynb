{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Theory Analysis: Professional Identity & AI\n",
    "\n",
    "## Research Question\n",
    "**\"How do professionals negotiate maintaining their identity and legitimacy in the face of a tool that threatens to replace them?\"**\n",
    "\n",
    "### Method: Gioia Methodology\n",
    "1. First-order codes (from informant language)\n",
    "2. Second-order themes (researcher concepts)\n",
    "3. Aggregate dimensions (theoretical constructs)\n",
    "4. Theoretical model construction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install -q google-generativeai datasets pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "print(\"‚úÖ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Important:** Ajoute ta cl√© API dans les Secrets Colab (üîë √† gauche) :\n",
    "- Nom: `GOOGLE_API_KEY`\n",
    "- Valeur: ta cl√© API Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - Modifier ici si besoin\n",
    "# ============================================\n",
    "\n",
    "# Cl√© API depuis les Secrets Colab\n",
    "from google.colab import userdata\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "\n",
    "# Mod√®le\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "# Question de recherche\n",
    "RESEARCH_QUESTION = \"\"\"How do professionals negotiate maintaining their identity and legitimacy \n",
    "in the face of a tool that threatens to replace them?\"\"\"\n",
    "\n",
    "# Nombre d'interviews √† analyser (None = toutes)\n",
    "SAMPLE_SIZE = 50\n",
    "\n",
    "# Configuration API\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(f\"‚úÖ Configuration OK\")\n",
    "print(f\"   Mod√®le: {MODEL_NAME}\")\n",
    "print(f\"   Interviews: {SAMPLE_SIZE or 'Toutes'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "print(\"üì• Chargement du dataset...\")\n",
    "dataset = load_dataset(\"Anthropic/AnthropicInterviewer\", split=\"workforce\")\n",
    "print(f\"‚úÖ {len(dataset)} interviews charg√©es\")\n",
    "print(f\"üìã Colonnes: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les interviews\n",
    "def prepare_interviews(dataset, sample_size=None):\n",
    "    interviews = []\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    if sample_size and sample_size < len(dataset):\n",
    "        indices = random.sample(indices, sample_size)\n",
    "    \n",
    "    for idx in indices:\n",
    "        item = dataset[idx]\n",
    "        content = \"\"\n",
    "        \n",
    "        # Extraire le texte\n",
    "        if isinstance(item, dict):\n",
    "            for key in ['text', 'content', 'transcript', 'conversation', 'messages']:\n",
    "                if key in item and item[key]:\n",
    "                    val = item[key]\n",
    "                    if isinstance(val, str):\n",
    "                        content = val\n",
    "                    elif isinstance(val, list):\n",
    "                        content = \"\\n\".join([\n",
    "                            f\"{m.get('role', '')}: {m.get('content', str(m))}\" \n",
    "                            if isinstance(m, dict) else str(m) \n",
    "                            for m in val\n",
    "                        ])\n",
    "                    break\n",
    "            if not content:\n",
    "                content = json.dumps(item, ensure_ascii=False)\n",
    "        else:\n",
    "            content = str(item)\n",
    "        \n",
    "        # Nettoyer le texte\n",
    "        content = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', content)\n",
    "        \n",
    "        interviews.append({'id': idx, 'content': content})\n",
    "    \n",
    "    return interviews\n",
    "\n",
    "interviews = prepare_interviews(dataset, SAMPLE_SIZE)\n",
    "print(f\"\\n‚úÖ {len(interviews)} interviews pr√©par√©es\")\n",
    "print(f\"üìè Longueur moyenne: {sum(len(i['content']) for i in interviews) // len(interviews)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Service Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiService:\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.request_count = 0\n",
    "    \n",
    "    def generate(self, prompt: str, temperature: float = 0.7, max_retries: int = 3) -> str:\n",
    "        self.request_count += 1\n",
    "        if self.request_count % 10 == 0:\n",
    "            time.sleep(1)\n",
    "        \n",
    "        config = genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=8192,\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt, generation_config=config)\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                wait = (attempt + 1) * 5\n",
    "                print(f\"‚ö†Ô∏è Retry {attempt+1}/{max_retries} dans {wait}s: {str(e)[:80]}\")\n",
    "                time.sleep(wait)\n",
    "        return \"\"\n",
    "    \n",
    "    def generate_json(self, prompt: str) -> Any:\n",
    "        response = self.generate(prompt + \"\\n\\nRespond ONLY with valid JSON.\", temperature=0.3)\n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        # Nettoyer\n",
    "        cleaned = response.strip()\n",
    "        for prefix in ['```json', '```']:\n",
    "            if cleaned.startswith(prefix):\n",
    "                cleaned = cleaned[len(prefix):]\n",
    "        if cleaned.endswith('```'):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        try:\n",
    "            return json.loads(cleaned.strip())\n",
    "        except:\n",
    "            match = re.search(r'[\\[\\{].*[\\]\\}]', cleaned, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    return json.loads(match.group())\n",
    "                except:\n",
    "                    pass\n",
    "            print(f\"‚ö†Ô∏è JSON parse error\")\n",
    "            return None\n",
    "\n",
    "gemini = GeminiService()\n",
    "print(\"‚úÖ GeminiService OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse Gioia - √âtape 1: First-Order Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def code_interview(interview: Dict) -> List[str]:\n    \"\"\"Code une interview individuelle - TEXTE COMPLET.\"\"\"\n    \n    # Pas de limite - on envoie l'interview compl√®te\n    content = interview['content']\n    \n    prompt = f\"\"\"You are an expert qualitative researcher using Gioia methodology.\n\nRESEARCH QUESTION: {RESEARCH_QUESTION}\n\nINTERVIEW (complete transcript):\n{content}\n\nAnalyze the ENTIRE interview and identify 5-15 FIRST-ORDER CODES related to:\n- Professional identity and self-definition\n- Perceived threats from AI\n- Coping and adaptation strategies  \n- Legitimacy claims\n- Identity negotiation\n\nUse language close to the informant's words.\n\nRespond with JSON array: [\"code 1\", \"code 2\", \"code 3\"]\"\"\"\n    \n    result = gemini.generate_json(prompt)\n    return result if isinstance(result, list) else []\n\n# Ex√©cuter le codage\nprint(\"üìù STEP 1: First-Order Coding\")\nprint(f\"   {len(interviews)} interviews √† coder (texte complet)...\\n\")\n\n# Afficher les stats de longueur\nlengths = [len(i['content']) for i in interviews]\nprint(f\"   üìè Longueur min: {min(lengths):,} chars\")\nprint(f\"   üìè Longueur max: {max(lengths):,} chars\")\nprint(f\"   üìè Longueur moyenne: {sum(lengths)//len(lengths):,} chars\\n\")\n\nall_codes = []\nfor interview in tqdm(interviews, desc=\"Coding\"):\n    codes = code_interview(interview)\n    interview['codes'] = codes\n    all_codes.extend(codes)\n    time.sleep(0.3)  # Rate limiting\n\nunique_codes = list(set(all_codes))\nprint(f\"\\n‚úÖ {len(unique_codes)} codes uniques identifi√©s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse Gioia - √âtape 2: Second-Order Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù STEP 2: Second-Order Coding\")\n",
    "print(f\"   Regroupement de {len(unique_codes)} codes...\\n\")\n",
    "\n",
    "# Limiter pour √©viter les erreurs\n",
    "codes_sample = unique_codes[:150]\n",
    "\n",
    "prompt = f\"\"\"You are an expert qualitative researcher using Gioia methodology.\n",
    "\n",
    "RESEARCH QUESTION: {RESEARCH_QUESTION}\n",
    "\n",
    "FIRST-ORDER CODES ({len(codes_sample)}):\n",
    "{json.dumps(codes_sample, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Group these into 6-10 SECOND-ORDER THEMES.\n",
    "Themes should capture:\n",
    "- Identity maintenance strategies\n",
    "- Threat perception patterns\n",
    "- Legitimacy construction\n",
    "- Adaptation behaviors\n",
    "- Professional boundary work\n",
    "\n",
    "Respond with JSON: {{\"Theme Name\": [\"code1\", \"code2\"], \"Another Theme\": [\"code3\"]}}\"\"\"\n",
    "\n",
    "second_order_themes = gemini.generate_json(prompt)\n",
    "\n",
    "if second_order_themes:\n",
    "    print(f\"‚úÖ {len(second_order_themes)} themes cr√©√©s:\\n\")\n",
    "    for theme, codes in second_order_themes.items():\n",
    "        print(f\"   üè∑Ô∏è {theme} ({len(codes)} codes)\")\n",
    "else:\n",
    "    print(\"‚ùå Erreur - r√©ex√©cute cette cellule\")\n",
    "    second_order_themes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse Gioia - √âtape 3: Aggregate Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù STEP 3: Aggregate Dimensions\")\n",
    "\n",
    "if not second_order_themes:\n",
    "    print(\"‚ùå Pas de themes - ex√©cute d'abord l'√©tape 2\")\n",
    "else:\n",
    "    prompt = f\"\"\"You are an expert qualitative researcher.\n",
    "\n",
    "RESEARCH QUESTION: {RESEARCH_QUESTION}\n",
    "\n",
    "SECOND-ORDER THEMES:\n",
    "{json.dumps(list(second_order_themes.keys()), indent=2)}\n",
    "\n",
    "Aggregate into 3-5 AGGREGATE DIMENSIONS.\n",
    "These are high-level theoretical constructs that answer the research question.\n",
    "\n",
    "Respond with JSON: {{\"Dimension Name\": [\"Theme A\", \"Theme B\"]}}\"\"\"\n",
    "\n",
    "    aggregate_dimensions = gemini.generate_json(prompt)\n",
    "    \n",
    "    if aggregate_dimensions:\n",
    "        print(f\"\\n‚úÖ {len(aggregate_dimensions)} dimensions cr√©√©es:\\n\")\n",
    "        for dim, themes in aggregate_dimensions.items():\n",
    "            print(f\"   üì¶ {dim}\")\n",
    "            for t in themes:\n",
    "                print(f\"      ‚îî‚îÄ {t}\")\n",
    "    else:\n",
    "        print(\"‚ùå Erreur - r√©ex√©cute cette cellule\")\n",
    "        aggregate_dimensions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. R√©sum√© de l'Analyse Gioia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler les r√©sultats\n",
    "results = {\n",
    "    'interviews': interviews,\n",
    "    'first_order_codes': unique_codes,\n",
    "    'second_order_themes': second_order_themes,\n",
    "    'aggregate_dimensions': aggregate_dimensions\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä GIOIA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìã Interviews: {len(results['interviews'])}\")\n",
    "print(f\"üìù First-order codes: {len(results['first_order_codes'])}\")\n",
    "print(f\"üè∑Ô∏è Second-order themes: {len(results['second_order_themes'])}\")\n",
    "print(f\"üì¶ Aggregate dimensions: {len(results['aggregate_dimensions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation tableau Gioia\n",
    "def display_gioia_table(results):\n",
    "    dimensions = results.get('aggregate_dimensions', {})\n",
    "    themes = results.get('second_order_themes', {})\n",
    "    \n",
    "    html = \"\"\"<style>\n",
    "    .gioia {border-collapse: collapse; width: 100%; font-family: Arial;}\n",
    "    .gioia th, .gioia td {border: 1px solid #ddd; padding: 10px; text-align: left; vertical-align: top;}\n",
    "    .gioia th {background: #4a90d9; color: white;}\n",
    "    .dim {background: #e8f4e8; font-weight: bold;}\n",
    "    .theme {background: #fff8e8;}\n",
    "    .codes {font-size: 11px; color: #555;}\n",
    "    </style>\n",
    "    <h3>üìä Gioia Data Structure</h3>\n",
    "    <table class='gioia'>\n",
    "    <tr><th>1st Order Codes</th><th>2nd Order Themes</th><th>Aggregate Dimensions</th></tr>\"\"\"\n",
    "    \n",
    "    for dim_name, dim_themes in dimensions.items():\n",
    "        first = True\n",
    "        for theme in dim_themes:\n",
    "            codes = themes.get(theme, [])\n",
    "            codes_html = \"<br>\".join([f\"‚Ä¢ {c}\" for c in codes[:5]])\n",
    "            if len(codes) > 5:\n",
    "                codes_html += f\"<br><i>+{len(codes)-5} more</i>\"\n",
    "            \n",
    "            html += f\"<tr><td class='codes'>{codes_html}</td><td class='theme'>{theme}</td>\"\n",
    "            if first:\n",
    "                html += f\"<td class='dim' rowspan='{len(dim_themes)}'>{dim_name}</td>\"\n",
    "                first = False\n",
    "            html += \"</tr>\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "display_gioia_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Construction du Mod√®le Th√©orique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è Building Theoretical Model...\\n\")\n",
    "\n",
    "prompt = f\"\"\"You are a qualitative research expert building grounded theory.\n",
    "\n",
    "RESEARCH QUESTION: {RESEARCH_QUESTION}\n",
    "\n",
    "GIOIA ANALYSIS RESULTS:\n",
    "\n",
    "Aggregate Dimensions: {json.dumps(aggregate_dimensions, indent=2)}\n",
    "\n",
    "Second-Order Themes: {json.dumps(list(second_order_themes.keys()), indent=2)}\n",
    "\n",
    "Construct a THEORETICAL MODEL with:\n",
    "1. Model name\n",
    "2. Core argument (2-3 sentences)\n",
    "3. Key constructs with definitions\n",
    "4. Testable propositions (P1, P2, etc.)\n",
    "5. Theoretical contributions\n",
    "6. Practical implications\n",
    "\n",
    "Connect to: identity theory, legitimacy theory, technology acceptance, boundary work.\n",
    "\n",
    "Respond with JSON:\n",
    "{{\n",
    "  \"model_name\": \"...\",\n",
    "  \"core_argument\": \"...\",\n",
    "  \"constructs\": [{{\"name\": \"...\", \"definition\": \"...\", \"type\": \"independent/dependent/mediator\"}}],\n",
    "  \"propositions\": [\"P1: ...\", \"P2: ...\"],\n",
    "  \"theoretical_contributions\": [\"...\"],\n",
    "  \"practical_implications\": [\"...\"]\n",
    "}}\"\"\"\n",
    "\n",
    "theoretical_model = gemini.generate_json(prompt)\n",
    "\n",
    "if theoretical_model:\n",
    "    print(\"‚úÖ Model constructed!\")\n",
    "else:\n",
    "    print(\"‚ùå Error - retry this cell\")\n",
    "    theoretical_model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le mod√®le\n",
    "if theoretical_model:\n",
    "    md = f\"\"\"# üß† {theoretical_model.get('model_name', 'Theoretical Model')}\n",
    "\n",
    "## Core Argument\n",
    "{theoretical_model.get('core_argument', 'N/A')}\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Constructs\n",
    "\"\"\"\n",
    "    for c in theoretical_model.get('constructs', []):\n",
    "        md += f\"\\n### {c.get('name')} *({c.get('type', 'construct')})*\\n\"\n",
    "        md += f\"{c.get('definition', '')}\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Propositions\\n\"\n",
    "    for p in theoretical_model.get('propositions', []):\n",
    "        md += f\"\\n**{p}**\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\\n## Theoretical Contributions\\n\"\n",
    "    for c in theoretical_model.get('theoretical_contributions', []):\n",
    "        md += f\"- {c}\\n\"\n",
    "    \n",
    "    md += \"\\n## Practical Implications\\n\"\n",
    "    for i in theoretical_model.get('practical_implications', []):\n",
    "        md += f\"- {i}\\n\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "else:\n",
    "    print(\"‚ùå No model - run previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualisation (Mermaid Diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if theoretical_model:\n",
    "    print(\"üìä Generating diagram...\")\n",
    "    \n",
    "    prompt = f\"\"\"Create a Mermaid flowchart for this model:\n",
    "{json.dumps(theoretical_model, indent=2)}\n",
    "\n",
    "Use graph LR. Show constructs as nodes, propositions as arrows.\n",
    "Return ONLY Mermaid code, no markdown.\"\"\"\n",
    "    \n",
    "    mermaid = gemini.generate(prompt, temperature=0.3)\n",
    "    mermaid = mermaid.replace('```mermaid', '').replace('```', '').strip()\n",
    "    \n",
    "    print(\"\\nüìà Mermaid Code:\")\n",
    "    print(mermaid)\n",
    "    \n",
    "    # Display\n",
    "    html = f\"\"\"\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n",
    "    <script>mermaid.initialize({{startOnLoad:true}});</script>\n",
    "    <div class=\"mermaid\">{mermaid}</div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Compiler tout\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'research_question': RESEARCH_QUESTION,\n",
    "        'dataset': 'Anthropic/AnthropicInterviewer',\n",
    "        'sample_size': len(interviews),\n",
    "        'model': MODEL_NAME,\n",
    "        'date': datetime.now().isoformat()\n",
    "    },\n",
    "    'gioia_analysis': {\n",
    "        'first_order_codes': unique_codes,\n",
    "        'second_order_themes': second_order_themes,\n",
    "        'aggregate_dimensions': aggregate_dimensions\n",
    "    },\n",
    "    'theoretical_model': theoretical_model,\n",
    "    'coded_interviews': [\n",
    "        {'id': i['id'], 'codes': i.get('codes', [])}\n",
    "        for i in interviews\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sauvegarder JSON\n",
    "filename = f\"gioia_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "# T√©l√©charger\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV des codes\n",
    "codes_data = []\n",
    "for theme, codes in second_order_themes.items():\n",
    "    dim = None\n",
    "    for d, ts in aggregate_dimensions.items():\n",
    "        if theme in ts:\n",
    "            dim = d\n",
    "            break\n",
    "    for code in codes:\n",
    "        codes_data.append({\n",
    "            'first_order_code': code,\n",
    "            'second_order_theme': theme,\n",
    "            'aggregate_dimension': dim\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(codes_data)\n",
    "csv_file = f\"gioia_codes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Saved: {csv_file}\")\n",
    "display(df.head(20))\n",
    "\n",
    "try:\n",
    "    files.download(csv_file)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "### M√©thode Gioia\n",
    "- **1st Order Codes**: Language proche des participants\n",
    "- **2nd Order Themes**: Concepts abstraits du chercheur\n",
    "- **Aggregate Dimensions**: Construits th√©oriques\n",
    "\n",
    "### R√©f√©rence\n",
    "Gioia, D. A., Corley, K. G., & Hamilton, A. L. (2013). Seeking qualitative rigor in inductive research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}